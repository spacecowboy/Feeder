<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:wfw="http://wellformedweb.org/CommentAPI/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
    xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
    xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
>

    <channel>
        <title>the morning paper</title>
        <atom:link href="https://blog.acolyer.org/feed/" rel="self" type="application/rss+xml" />
        <link>https://blog.acolyer.org</link>
        <description>an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer</description>
        <lastBuildDate>Tue, 07 Mar 2017 16:53:21 +0000</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>
        <generator>http://wordpress.com/</generator>
        <cloud domain='blog.acolyer.org' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
        <image>
            <url>https://secure.gravatar.com/blavatar/09326a066a08237015d6b84f026d36ae?s=96&#038;d=https%3A%2F%2Fs2.wp.com%2Fi%2Fbuttonw-com.png</url>
            <title>the morning paper</title>
            <link>https://blog.acolyer.org</link>
        </image>
        <atom:link rel="search" type="application/opensearchdescription+xml" href="https://blog.acolyer.org/osd.xml" title="the morning paper" />
        <atom:link rel='hub' href='https://blog.acolyer.org/?pushpress=hub'/>
        <item>
            <title>Thou shalt not depend on me: analysing the use of outdated JavaScript libraries on the web</title>
            <link>https://blog.acolyer.org/2017/03/07/thou-shalt-not-depend-on-me-analysing-the-use-of-outdated-javascript-libraries-on-the-web/</link>
            <comments>https://blog.acolyer.org/2017/03/07/thou-shalt-not-depend-on-me-analysing-the-use-of-outdated-javascript-libraries-on-the-web/#comments</comments>
            <pubDate>Tue, 07 Mar 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Uncategorized]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4116</guid>
            <description><![CDATA[Thou shalt not depend on me: analysing the use of outdated JavaScript libraries on the web Lauinger et al., NDSS 2017 Just based on the paper title alone, if you had to guess what the situation is with outdated JavaScript libraries on the web, you&#8217;d probably guess it was pretty bad. It turns out it&#8217;s [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4116&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p><a href="http://www.ccs.neu.edu/home/arshad/publications/ndss2017jslibs.pdf">Thou shalt not depend on me: analysing the use of outdated JavaScript libraries on the web</a>  Lauinger et al., <em>NDSS 2017</em></p>
<p>Just based on the paper title alone, if you had to guess what the situation is with outdated JavaScript libraries on the web, you&#8217;d probably guess it was pretty bad. It turns out it&#8217;s very bad indeed, and we&#8217;ve created a huge mess with nowhere near enough attention being paid to the issue. The first step towards better solutions is recognising that we have a problem, and Lauinger et al., do a tremendous job in that regard.</p>
<blockquote><p>
  In this paper, we conduct the first comprehensive study of client-side JavaScript library usage and the resulting security implications across the Web. Using data from over 133K websites, we show that 37% of them include at least one library with a known vulnerability; the time lag behind the newest release of a library is measured in the order of years.
</p></blockquote>
<p>For example, 36.7% of jQuery includes, 40.1% of Angular, and an astonishing 86.6% of Handlebars includes use a vulnerable version. Those are headline grabbing numbers, but when you go deeper it turns out there&#8217;s no quick fix in sight because the root causes are systemic in the JavaScript ecosystem:</p>
<blockquote><p>
  Perhaps our most sobering finding is practical evidence that the JavaScript library ecosystem is complex, unorganised, and quite “ad hoc” with respect to security. There are no reliable vulnerability databases, no security mailing lists maintained by library vendors, few or no details on security issues in release notes, and often, it is difficult to determine which versions of a library are affected by a specific reported vulnerability.
</p></blockquote>
<p>Let&#8217;s briefly look at how the authors collected their data before diving deeper into what the results themselves tell us.</p>
<h3>Data gathering methodology</h3>
<p>The team crawled the Alexa Top 75K websites (ALEXA) and also a random sample of 75K websites drawn from the <code>.com</code> domain (COM). This enables a comparison of JavaScript usage across popular and unpopular websites.</p>
<p>Figuring out which JavaScript libraries were actually in use, and their versions, took quite a bit of work. As did figuring out which versions contain vulnerabilities. Details of the tools and techniques used for this can be found in the paper, in brief it involved:</p>
<ul>
<li>Manually constructing a catalogue of all releases versions of the 72 most popular open source libraries (using popularity statistics from <a href="https://bower.io/">Bower</a> and <a href="https://wappalyzer.com/">Wappalyzer</a>). </li>
<li>Using static and dynamic analysis techniques to cope with the fact that developers often reformat, restructure, or append code making it difficult to detect library usage in the wild</li>
<li>Implementing an in-browser causality tracker to understand <em>why</em> specific libraries are loaded by a given site.</li>
</ul>
<h3>Vulnerabilities</h3>
<blockquote><p>
  The last step towards building our catalogue is aggregating vulnerability information for our 72 JavaScript libraries. Unfortunately, there is no centralised database of vulnerabilities in JavaScript libraries; instead, we manually compile vulnerability information from the Open Source Vulnerability Database (OSVDB), the National Vulnerability Database (NVD), public bug trackers, GitHub comments, blog posts, and the vulnerabilities detected by Retire.js. Overall, we are able to obtain systematically documented details of vulnerabilities for 11 of the JavaScript libraries in our catalogue.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-1.jpeg?w=640" alt="" /></p>
<h3>Causality trees</h3>
<p>To figure out why a certain library is being loaded, the authors develop a <em>causality tree</em> chrome extension. Nodes in the tree are snapshots of elements in the DOM at a specific point in time, and edges denote &#8220;created by&#8221; relationships.</p>
<p>For example:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-2.jpeg?w=640" alt="" /></p>
<p>and</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-12.jpeg?w=520" alt="" /></p>
<p>The median causality tree in ALEXA contains 133 nodes, and the median depth is 4 inclusions.</p>
<h3>JavaScript library market share</h3>
<p>jQuery remains by far the most popular library, found on 84.5% of ALEXA sites.  Note also SWFObject (Adobe Flash) still used on 10.7% of ALEXA sites despite being discontinued in 2013.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-table-1.jpeg?w=520" alt="" /></p>
<p>When externally loaded, scripts are mostly loaded from CDNs (note also the domain parking sites popping up in the long tail of the COM sites):</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-table-3.jpeg?w=520" alt="" /></p>
<p>Overall though, there seems to be a pretty even split between internally hosted and CDN-delivered script libraries:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-table-iv.jpeg?w=520" alt="" /></p>
<h3>Distribution of vulnerable libraries</h3>
<p><strong>37.8%</strong> of ALEXA sites use at least one library version known to the authors to be vulnerable.</p>
<blockquote><p>
  Highly-ranked websites tend to be less likely to include vulnerable libraries, but they are also less likely to include any detected library at all. Towards the lower ranks, both curves increase at a similar pace until they stabilise. While only 21 % of the Top 100 websites use a known vulnerable library, this percentage increases to 32.2 % in the Top 1 k before it stabilises in the Top 5 k and remains around the overall average of 37.8 % for all 75 k websites.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-9.jpeg?w=640" alt="" /></p>
<p>37.4% of the COM sites use at least one vulnerable library. Within the ALEXA grouping, <em>financial and government sites are the worst</em>, with 52% and 50% of sites containing vulnerable libraries respectively.</p>
<p>The following table shows the percentage of vulnerable copies in the wild for jQuery, jQ-UI, Angular, Handlebars, and YUI 3.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/js-libs-table-v.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/js-libs-table-v.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<blockquote><p>
  In ALEXA, 36.7% of jQuery inclusions are known vulnerable, when at most one inclusion of a specific library version is counted per site. Angular has 40.1% vulnerable inclusions, Handlebars has 86.6%, and YUI 3 has 87.3% (it is not maintained any more). These numbers illustrate that inclusions of known vulnerable versions can make up even a majority of all inclusions of a library.
</p></blockquote>
<p>Many libraries it turns out are not <em>directly</em> included by the site, but are pulled in by other libraries that are. &#8220;<em>Library inclusions by ad, widget, or tracker code appear to be more vulnerable than unrelated inclusions.</em>&#8221;</p>
<p>Another interesting analysis is the <em>age</em> of the included libraries &#8211; the data clearly shows that the majority of web sites use library versions released a long time ago, suggesting that developers rarely update their library dependencies once they have deployed a site. 61.7% of ALEXA sites are at least one patch version behind on one of their included libraries, and the median ALEXA site uses a version released <strong>1,177 days</strong> before the newest release of the library. Literally <em>years</em> out of date.</p>
<h3>Duplicate inclusions</h3>
<p>If you like a little non-determinism in your web app (I find it always make debugging <em>much</em> more exciting <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" alt="😉" class="wp-smiley" style="height: 1em; max-height: 1em;" /> ), then another interesting find is that many sites include the same libraries (and multiple versions thereof) many times over!</p>
<blockquote><p>
  We discuss some examples using jQuery as a case study. About 20.7 % of the websites including jQuery in ALEXA (17.2 % in COM) do so two or more times. While it may be necessary to include a library multiple times within different documents from different origins, 4.2 % of websites using jQuery in ALEXA include the same version of the library two or more times into the same document (5.1 % in COM), and 10.9 % (5.7 %) include two or more different versions of jQuery into the same document. Since jQuery registers itself as a window-global variable, unless special steps are taken only the last loaded and executed instance can be used by client code. For asynchronously included instances, it may even be difficult to predict which version will prevail in the end.
</p></blockquote>
<h3>What can be done?</h3>
<p>So where does all this leave us?</p>
<blockquote><p>
  From a remediation perspective, the picture painted by our data is bleak. We observe that only very small fraction of potentially vulnerable sites (2.8 % in ALEXA, 1.6 % in COM) could become free of vulnerabilities by applying patch-level updates, i.e., an update of the least significant version component, such as from 1.2.3 to 1.2.4, which would generally be expected to be backwards compatible. The vast majority of sites would need to install at least one library with a more recent major or minor version, which might necessitate additional code changes due to incompatibilities.
</p></blockquote>
<p>Version aliasing could potentially help (specifying only a library prefix, and allowing the CDN to return the latest version), but only a tiny percentage of sites use it (would <em>you</em> trust the developers of those libraries not to break your site, completely outside of your control?).  Note that:</p>
<blockquote><p>
  Google recently discontinued this service, citing caching issues and &#8220;lack of compatibility between even minor versions.&#8221;
</p></blockquote>
<p>We need proper dependency management which makes it clear which versions of libraries are being used, coupled with knowledge within the supply chain of vulnerabilities. &#8220;<em>This functionality would ideally be integrated into the dependency management system of the platform so that a warning can be shown each time a developer includes a known vulnerable component from the central repository.</em>&#8221;</p>
<p>Of course, that can only work if we have some way of figuring out which libraries are vulnerable in the first place. The state of the practice here is pretty damning :</p>
<blockquote><p>
  Unfortunately, security does not appear to be a priority in the JavaScript library ecosystem. Popular vulnerability databases contain nearly no entries regarding JavaScript libraries. During this entire work, we did not encounter a single popular library that had a dedicated mailing list for security announcements (in fact, most libraries we investigated did not have a mailing list for announcements at all). Furthermore, only a few JavaScript library developers provide a dedicated email address where users can submit vulnerability reports&#8230;.
</p></blockquote>
<p>Consider jQuery, one of the most widely used libraries:</p>
<blockquote><p>
  Although jQuery is an immensely popular library, the fact that searching for “security” or “vulnerability” in the official learning centre returns “Apologies, but nothing matched your search criteria” is an excellent summary of the state of JavaScript library security on the Internet, circa August 2016
</p></blockquote>
<p>Since we also know that many libraries are only indirectly loaded by web sites, and are brought in through third-party components such as advertising, tracking, and social media code, even web developers trying to stay on top of the situation may be unaware that they are indirectly introducing vulnerable code into their websites.</p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4116&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/03/07/thou-shalt-not-depend-on-me-analysing-the-use-of-outdated-javascript-libraries-on-the-web/feed/</wfw:commentRss>
            <slash:comments>6</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-2.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-12.jpeg?w=520" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-table-1.jpeg?w=520" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-table-3.jpeg?w=520" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-table-iv.jpeg?w=520" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-fig-9.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/js-libs-table-v.jpeg?w=640" medium="image" />
        </item>
        <item>
            <title>HopFS: Scaling hierarchical file system metadata using NewSQL databases</title>
            <link>https://blog.acolyer.org/2017/03/06/hopfs-scaling-hierarchical-file-system-metadata-using-newsql-databases/</link>
            <comments>https://blog.acolyer.org/2017/03/06/hopfs-scaling-hierarchical-file-system-metadata-using-newsql-databases/#comments</comments>
            <pubDate>Mon, 06 Mar 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Uncategorized]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4106</guid>
            <description><![CDATA[HopFS: Scaling hierarchical file system metadata using NewSQL databases Niazi et al., FAST 2017 If you&#8217;re working with big data and Hadoop, this one paper could repay your investment in The Morning Paper many times over (ok, The Morning Paper is free &#8211; but you do pay with your time to read it). You know [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4106&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p><a href="https://www.usenix.org/system/files/conference/fast17/fast17-niazi.pdf">HopFS: Scaling hierarchical file system metadata using NewSQL databases</a> Niazi et al., <em>FAST 2017</em></p>
<p>If you&#8217;re working with big data and Hadoop, this one paper could repay your investment in The Morning Paper many times over (ok, The Morning Paper is free &#8211; but you do pay with your time to read it). You know that moment when you&#8217;re working on a code base and someone says &#8220;why don&#8217;t we replace all this complex home-grown code and infrastructure with this pre-existing solution&#8230;?&#8221; That!</p>
<p>Here&#8217;s the big idea &#8211; for large HDFS installations, the single node in-memory metadata service is the bottleneck. So why not replace the implementation with a NewSQL database and spread the load across multiple nodes? In this instance, MySQL Cluster was used, but that&#8217;s pluggable. Of course, there will be some important design issues to address, but the authors do a neat job of solving these.  I especially like that the paper is grounded in real world workloads from Spotify (and we get some nice insights into the scale of data at Spotify too as a bonus).</p>
<p>All very nice, but what difference does HopFS (a drop-in replacement) make in the real world? How about:</p>
<ul>
<li>Enabling an order-of-magnitude larger clusters</li>
<li>Improving cluster throughput by an order-of-magnitude (16x &#8211; 37x)</li>
<li>Lower latency when using large numbers of concurrent clients</li>
<li>No downtime during failover</li>
</ul>
<p>Think of the capital and operational costs of having to stand up a second large-scale Hadoop cluster because your existing one is capacity or throughput limited. HopFS is a huge win if it eliminates your need to do that.</p>
<h3>NameNodes in Apache Hadoop vs HopFS</h3>
<p>In vanilla Apache Hadoop, HDFS metadata is stored on the heap of a single Java process, the Active NameNode (ANN). The ANN logs changes to journal servers using quorum based replication. The change log is asynchronously replicated to a Standby NameNode. ZooKeeper is used to determine which node is active, and to coordinate failover from the active to the standby namenode. Datanodes connect to both active and standby namenodes.</p>
<blockquote><p>
  In HDFS the amount of metadata is quite low relative to file data. There is approximately 1 gigabyte of metadata for every petabyte of file system data. Spotify’s HDFS cluster has 1600+ nodes, storing 60 petabytes of data, but its metadata fits in 140 gigabytes Java Virtual Machine (JVM) heap. The extra heap space is taken by temporary objects, RPC request queues and secondary metadata required for the maintenance of the file system. However, current trends are towards even larger HDFS clusters (Facebook has HDFS clusters with more than 100 petabytes of data), but current JVM garbage collection technology does not permit very large heap sizes, as the application pauses caused by the JVM garbage collector affects the operations of HDFS. As such, <em>JVM garbage collection technology and the monolithic architecture of the HDFS namenode are now the scalability bottlenecks for Hadoop</em>.
</p></blockquote>
<p>HopFS is a drop-in replacement for HDFS, based on HDFS v2.0.4. Instead of a single in-memory process, it provides a <em>scale-out</em> metadata service. Multiple stateless namenode processes handle client requests and store data in an external distributed database, MySQL Cluster. References to NDB throughout the paper refer to the Network DataBase storage engine for MySQL Cluster.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-1.jpeg?w=640" alt="" /></p>
<h3>Partitioning</h3>
<p>HDFS metadata contains information on inodes, blocks, replicas, quotas, leases and mappings (dirs to files, files to blocks, blocks to replicas).</p>
<blockquote><p>
  When metadata is distributed, an application defined partitioning scheme is needed to shard the metadata and a consensus protocol is required to ensure metadata integrity for operations that cross shards.
</p></blockquote>
<p>The chosen partition scheme is based on a study of the relative frequency of operations in production deployments. Common file system operations (primary key, batched primary key, and partition pruned index scans) can be implemented using only low cost database operations.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-table-1.jpeg?w=480" alt="" /></p>
<p>File system metadata is stored in tables, with a directory inode represented by a single row in the <em>Inode</em> table. File inodes have more associated metadata, that is stored in a collected of related tables.</p>
<blockquote><p>
  With the exception of hotspots, HopFS partitions inodes by their parents&#8217; inode IDs, resulting in inodes with the same parent inode being stored on the same database shard.
</p></blockquote>
<p>A hinting mechanism allows e.g.,  the transaction for listing files in a directory to be initiated on a transaction coordinator on the shard holding the child inodes for the directory.</p>
<p>Hotspots are simply inodes that receive a high proportion of file system operations. The <em>root</em> inode is immutable and cached at all namenodes. The immediate children of top level directories receive special treatment to avoid them becoming hotspots &#8211; they are pseudo-randomly partitioned by hashing the names of the children. By default just the first two levels of the hierarchy receive this treatment.</p>
<h3>Transactions</h3>
<p>HopFS uses transactions for all operations, coupled with row-level locking to serialize conflicting inode operations. Taking multiple locks in a transaction can lead to deadlocks and timeouts unless care is take to avoid cycles and upgrade deadlocks.</p>
<p>To avoid cycle deadlocks, HopFS reimplemented all inode operations to acquire locks on the metadata in the same total order. (Root to leaves, left-ordered depth-first search). Upgrade deadlocks are prevented by acquiring all locks at the start of the transaction.</p>
<blockquote><p>
  In HDFS, many inode operations contain read operations followed by write operations on the same metadata. When translated into database operations within the same transaction, this results in deadlocking due to lock upgrades from read to exclusive locks. We have examined all locks acquired by the inode operations, and re-implemented them so that all data needed in a transaction is read only once at the start of the transaction at the strongest lock level that could be needed during the transaction, thus preventing lock upgrades.
</p></blockquote>
<p>Inode operations proceed in three phases: lock, execute, and update.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-4.jpeg?w=520" alt="" /></p>
<p>Operations on large directories (e.g, containing millions of inodes) are too large to fit in a single transaction. A <em>subtree operations protocol</em> instead performs such operations incrementally in a series of transactions.</p>
<blockquote><p>
  We serialize subtree operations by ensuring that all ongoing inode and subtree operations in a subtree complete before a newly requested subtree operation is executed. We implement this serialization property by enforcing the following invariants: (1) no new operations access the subtree until the operation completes, (2) the subtree is quiesced before the subtree operation starts, (3) no orphaned inodes or inconsistencies arise if failures occur.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-5.jpeg?w=480" alt="" /></p>
<h3>Evaluation</h3>
<p>For the evaluation, HopFS used NDB v7.5.3 deployed on 12 nodes configured to run 22 threads each, and with data replication degree 2. HDFS namenode suport was deployed on 5 servers: one active namenode, one standby namenode, and three journal nodes colocated with three ZooKeeper nodes. The benchmark used traces from Spotify&#8217;s 1600+ node cluster containing 60 Petabytes of data, 13 million directories, and 218 million files. This cluster runs on average 40,000 jobs a day from a variety of applications.</p>
<blockquote><p>
  Figure 6 (below) shows that, for our industrial workload, using 60 namenodes and 12 NDB nodes, HopsFS can perform 1.25 million operations per second delivering 16 times the throughput of HDFS. As discussed before in medium to large Hadoop clusters 5 to 8 servers are required to provide high availability for HDFS. With equivalent hardware (2 NDB nodes and 3 namenodes), HopsFS delivers ≈10% higher throughput than HDFS. HopsFS performance increases linearly as more namenodes nodes are added to the system
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-6.jpeg?w=640" alt="" /></p>
<p>HDFS reaches a file limit of about 470 million files due to constraints on the JVM heap size. HopFS needs about 1.5 times more memory in aggregate than HDFS to store metadata that is highly available, but it can scale to many more files&#8230;</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-table-3.jpeg?w=480" alt="" /></p>
<p>A saturation test explored the maximum throughput and scalability of each file system operation. &#8220;<em>In real deployments, the namenode often receives a deluge of the same file system operation type, for example, a big job that reads large amounts of data will generate a huge number of requests to read files and list directories.</em>&#8221;</p>
<p>In the results below, HopFS&#8217; results are displayed as a bar chart of stacked rectangles, each representing the increase in throughput when five new namenodes are added:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-7.jpeg?w=640" alt="" /></p>
<blockquote><p>
  HopFS outperforms HDFS for all file system operations and has significantly better performance than HDFS for the most common file system operations.
</p></blockquote>
<p>When it comes to latency, it is true that HopFS is slower than HDFS for single filesystem operations on unloaded namenodes. But start to scale up the workload and you can quickly see HopFS has the advantage:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-8.jpeg?w=480" alt="" /></p>
<blockquote><p>
  Large HDFS deployments may have tens of thousands of clients and the end-to-end latency observed by the clients increases as the file system operations wait in RPC call queues at the namenode. In contrast, HopFS can handle more concurrent clients while keeping operation latencies low.
</p></blockquote>
<p>HopFS provides much faster failover with no downtime too:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-10.jpeg?w=480" alt="" /></p>
<p>Surely there is <em>some</em> downside to HopFS?? Well, yes there is one: HopFS can only process about 30 block reports a second, whereas HDFS does 60.  But HopFS doesn&#8217;t need block reports as often as HDFS does, and with datanodes sending block reports every six hours it can still scale to exabyte sized clusters.</p>
<p>The bottom line:</p>
<blockquote><p>
  HopsFS is an open-source, highly available file system that scales out in both capacity and throughput by adding new namenodes and database nodes. HopsFS can store 37 times more metadata than HDFS and for a workload from Spotify HopsFS scales to handle 16 times the throughput of HDFS. HopsFS also has lower average latency for large number of concurrent clients, and no downtime during failover. Our architecture supports a pluggable database storage engine, and other NewSQL databases could be used.
</p></blockquote><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4106&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/03/06/hopfs-scaling-hierarchical-file-system-metadata-using-newsql-databases/feed/</wfw:commentRss>
            <slash:comments>5</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-table-1.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-4.jpeg?w=520" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-5.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-6.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-table-3.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-7.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-8.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/hopfs-fig-10.jpeg?w=480" medium="image" />
        </item>
        <item>
            <title>RNN models for image generation</title>
            <link>https://blog.acolyer.org/2017/03/03/rnn-models-for-image-generation/</link>
            <comments>https://blog.acolyer.org/2017/03/03/rnn-models-for-image-generation/#comments</comments>
            <pubDate>Fri, 03 Mar 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>
            <category><![CDATA[Uncategorized]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4061</guid>
            <description><![CDATA[Today we&#8217;re looking at the remaining papers from the unsupervised learning and generative networks section of the &#8216;top 100 awesome deep learning papers&#8216; collection. These are: DRAW: A recurrent neural network for image generation, Gregor et al., 2015 Pixel recurrent neural networks, van den Oord et al., 2016 Auto-encoding variational Bayes, Kingma &#38; Welling, 2014 [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4061&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>Today we&#8217;re looking at the remaining papers from the unsupervised learning and generative networks section of the &#8216;<a href="https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques">top 100 awesome deep learning papers</a>&#8216; collection. These are:</p>
<ul>
<li><a href="http://arxiv.org/pdf/1502.04623">DRAW: A recurrent neural network for image generation</a>, Gregor et al., 2015</li>
<li><a href="http://arxiv.org/pdf/1601.06759v2.pdf">Pixel recurrent neural networks</a>, van den Oord et al., 2016</li>
<li><a href="http://arxiv.org/pdf/1312.6114">Auto-encoding variational Bayes</a>, Kingma &amp; Welling, 2014</li>
</ul>
<h3>DRAW: A recurrent neural network for image generation</h3>
<p>The networks we looked at yesterday generate a complete image all at once. This &#8220;one shot&#8221; approach is hard to scale to large images. If you ask a person to draw some visual scene, they will typically do so in a sequential iterative fashion, working from rough outlines to detail, first on one part, then on another.</p>
<blockquote><p>
  The <em>Deep Recurrent Attentive Writer</em> (DRAW) architecture represents a shift towards a more natural form of image construction, in which parts of a scene are created independently from others, and approximate sketches are successively refined.
</p></blockquote>
<p>At its core, DRAW is a variational autoencoder (an autoencoder making use of variational inference techniques). The encoder and decoder however are both <em>RNNs</em> (LSTMs):</p>
<blockquote><p>
  &#8230; a <em>sequence</em> of code samples is exchanged between them; moreover, the encoder is privy to the decoder&#8217;s previous outputs, allowing it to tailor the codes it sends according to the decoder&#8217;s behavior so far.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/draw-fig-2-r.jpeg?w=480" alt="" /></p>
<p>The decoder&#8217;s outputs are successively added to the distribution that will ultimately generate the data, as opposed to emitting it in a single step. This much takes care of the <em>iterative</em> part of human image construction. To model the phenomenon of working first on one part of the image, and then on another, an <em>attention mechanism</em> is used to restrict both the input region observed by the encoder, and the output region modified by the decoder.</p>
<blockquote><p>
  In simple terms, the network decides at each time-step &#8220;where to read&#8221; and &#8220;where to write&#8221; as well as &#8220;what to write&#8221;.
</p></blockquote>
<p>The attention mechanism resembles the selective read and write operations of the <a href="https://blog.acolyer.org/2016/03/09/neural-turing-machines/">Neural Turing Machine</a> that we looked at last year, however it works in 2D. An array of 2D Guassian filters is applied to the image, which yields image patches of smoothly varying location and zoom.</p>
<blockquote><p>
  As illustrated [below], the N ×N grid of Gaussian filters is positioned on the image by specifying the co-ordinates of the grid centre and the stride distance between adjacent filters. The stride controls the ‘zoom’ of the patch; that is, the larger the stride, the larger an area of the original image will be visible in the attention patch, but the lower the effective resolution of the patch will be.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/draw-fig-3.jpeg?w=480" alt="" /></p>
<p>Once a DRAW network has been trained, an image can be generated by iteratively picking latent samples and running the decoder to update the canvas matrix. Here we can see how images evolve when a trained DRAW network generates MNIST digits:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/draw-fig-1.jpeg?w=480" alt="" /></p>
<p>The final generated digits are pretty much indistinguishable from the originals.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/draw-fig-6.jpeg?w=480" alt="" /></p>
<p>Here&#8217;s another set of generated images, this time from a network trained on a multi-digit Street View House Numbers dataset:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/draw-fig-9.jpeg?w=480" alt="" /></p>
<h3>Pixel recurrent neural networks</h3>
<p>In contrast to DRAW, Pixel RNNs use a distinctly un-human approach: they model the probability of raw pixel values. The goal of the work is to be able to model natural images on a large scale, but the authors also evaluated Pixel RNNs on good old MNIST, and reported the best result so far (including against DRAW). There are two variations of the basic idea in the paper, a full-fat Pixel RNN architecture, and a simpler Pixel CNN one &#8211; both are substantial.</p>
<p>A <em>Pixel RNN</em> network has up to twelve two-dimensional LSTM layers:</p>
<blockquote><p>
  These layers use LSTM units in their state and adopt a convolution to compute at once all the states along one of the spatial dimensions of the data.
</p></blockquote>
<p>To generate pixel <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x_i" title="x_i" class="latex" />, Pixel conditions on all the previously generated pixels left and above it.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-left.jpeg?w=320" alt="" /></p>
<p>There are two types of these layers: <em>Row LSTM layers</em> apply the convolution along each row of pixels; <em>Diagonal BiLSTM layers</em> apply the convolution along the diagonals of the image (like a bishop moving on a chessboard).</p>
<p>Here&#8217;s an illustration of the Row LSTM convolution with a kernel size of 3 (3 pixels wide). Notice how it fails to reach pixels on the far sides of the image in rows close to <img src="https://s0.wp.com/latex.php?latex=x_i&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="x_i" title="x_i" class="latex" />.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-centre.jpeg?w=320" alt="" /></p>
<p>In contrast, the Diagonal BiLSTM&#8217;s dependency field covers the entire available context in the image:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-right.jpeg?w=320" alt="" /></p>
<p>A <em>Pixel CNN</em> network is fully convolution and has up to fifteen layers, &#8220;<em>We observe that CNNs can also be used as a sequence model with a fixed dependency range, by using Masked convolutions.</em>&#8221;</p>
<p>With <em>Masked Convolution</em> the R,G,B channels for the current pixel are predicted separately. When predicting the R channel, only the pixels left and above can be used for conditioning. But when we predict the G channel, we can also use the predicted value for R at the current pixel. And when we predict the B value, we can use the predicted values for both R and G.</p>
<blockquote><p>
  To restrict connections in the network to these dependencies, we apply a mask to the input-to-state convolutions and to other purely convolutional layers in a PixelRNN. We use two types of masks that we indicate with mask A and mask B, as shown [below]. Mask A is applied only to the first convolutional layer in a PixelRNN and restricts the connections to those neighboring pixels and to those colors in the current pixels that have already been predicted. On the other hand, mask B is applied to all the subsequent input-to-state convolutional transitions and relaxes the restrictions of mask A by also allowing the connection from a color to itself.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-4-r.jpeg?w=320" alt="" /></p>
<p>The Pixel CNN is the fastest architecture, whereas Pixel RNNs with Diagonal BiLSTM layers perform the best in terms of generating likely images. For generation of larger images, <em>Multiscale</em> Pixel RNNs do even better.</p>
<blockquote><p>
  The Multi-Scale PixelRNN is composed of an unconditional PixelRNN and one or more conditional PixelRNNs. The unconditional network first generates a smaller s × s image just like a standard PixelRNN described above, where s is a smaller integer that divides n. The conditional network then takes the s × s image as an additional input and generates a larger n × n image.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-7.jpeg?w=640" alt="" /></p>
<p>One fun experiment that shows the power of the method is to occlude the lower part of an image, and then ask a PixelRNN to complete the image. Here are some example generated completions (with the original in the right-hand column for comparison):</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-1.jpeg?w=480" alt="" /></p>
<blockquote><p>
  Based on the samples and completions drawn from the models we can conclude that the PixelRNNs are able to model both spatially local and long-range correlations and are able to produce images that are sharp and coherent. Given that these models improve as we make them larger and that there is<br />
  practically unlimited data available to train on, more computation and larger models are likely to further improve the results.
</p></blockquote>
<h3>Auto-encoding variational Bayes</h3>
<p>The auto-encoding paper is short and dense, and I don&#8217;t feel able to add much value to it, so here&#8217;s a very short summary of what to expect if you go on to read it. The opening sentence sets the tone:</p>
<blockquote><p>
  How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? [Oh, and we want to do it with large datasets].
</p></blockquote>
<p>I bet you were wondering exactly that in the shower this morning! Let&#8217;s unpack this sentence to see what it&#8217;s all about:</p>
<ul>
<li>&#8216;<em>efficient</em> approximate inference and learning&#8217; simply says that we want a training approach which is not too costly to compute.</li>
<li>&#8216;<em>directed probabilistic models</em>&#8216; are graph models where the probability at some node is conditioned on the probabilities of other nodes, with directed edges in the graph from cause nodes to affected nodes.</li>
<li>&#8216;<em>continuous latent variables</em>&#8216; suggests that there are true hidden causes for the observed behaviour, which are not directly represented in our model (latent). These are real-valued (i.e., not discrete).</li>
<li>&#8216;<em>intractable posterior distributions</em>&#8216; means that we can&#8217;t calculate an exact answer for the output probability distribution, so we&#8217;ll need to <em>approximate</em> it.</li>
<li>The use of <em>large datasets</em> further stresses the need for efficiency.</li>
</ul>
<p>The authors demonstrate a simple and efficient differentiable (i.e., easily trainable) estimator that fits the bill, which they call Stochastic Gradient Variational Bayes (SGVB). Using this in an autoencoder leads to Auto-Encoding Variational Bayes (AEVB).</p>
<p>If you&#8217;re feeling brave and want to dig in (or your knowledge of variational Bayesian methods is better than mine!), then these wikipedia entries on <a href="https://en.m.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Bayesian methods</a> and <a href="https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> might come in handy.</p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4061&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/03/03/rnn-models-for-image-generation/feed/</wfw:commentRss>
            <slash:comments>1</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/draw-fig-2-r.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/draw-fig-3.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/draw-fig-1.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/draw-fig-6.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/draw-fig-9.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-left.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-centre.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-2-right.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-4-r.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-7.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/pixelrnn-fig-1.jpeg?w=480" medium="image" />
        </item>
        <item>
            <title>Unsupervised learning and GANs</title>
            <link>https://blog.acolyer.org/2017/03/02/unsupervised-learning-and-gans/</link>
            <comments>https://blog.acolyer.org/2017/03/02/unsupervised-learning-and-gans/#comments</comments>
            <pubDate>Thu, 02 Mar 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4059</guid>
            <description><![CDATA[Continuing our tour through some of the &#8216;top 100 awesome deep learning papers,&#8217; today we&#8217;re turning our attention to the unsupervised learning and generative networks section. I&#8217;ve split the papers here into two groups. Today we&#8217;ll be looking at: Building high-level features using large-scale unsupervised learning, Le et al., 2012 Generative Adversarial Nets, Goodfellow et [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4059&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>Continuing our tour through some of the &#8216;<a href="https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques">top 100 awesome deep learning papers</a>,&#8217; today we&#8217;re turning our attention to the unsupervised learning and generative networks section. I&#8217;ve split the papers here into two groups. Today we&#8217;ll be looking at:</p>
<ul>
<li><a href="http://arxiv.org/pdf/1112.6209">Building high-level features using large-scale unsupervised learning</a>, Le et al., 2012</li>
<li><a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Nets</a>, Goodfellow et al., 2014</li>
<li><a href="https://arxiv.org/pdf/1511.06434v2">Unsupervised representation learning with deep convolutional generative adversarial networks</a>, Radford et al., 2015</li>
<li><a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf">Improved techniques for training GANs</a>, Salimans et al., 2016</li>
</ul>
<h3>Building high-level features using large-scale unsupervised learning</h3>
<p>This is a fascinating paper. Consider an unsupervised learning scenario in which a deep <a href="https://en.m.wikipedia.org/wiki/Autoencoder">autoencoder</a> is fed a large number of images (the authors construct a training dataset by sampling frames from 10 million YouTube videos). Do the features learned by the encoder correspond in any way to the things that you and I might recognise as features? And are there even neurons that specialise for recognising certain types of object? If so, that would not only be really interesting in and of itself, but we could also figure out which objects fire in response to which objects, and so build a classifier.</p>
<blockquote><p>
  This work investigates the feasibility of building high-level features from only unlabeled data. A positive answer to this question will give rise to two significant results. Practically, this provides an inexpensive way to develop features from unlabeled data. But perhaps more importantly, it answers an intriguing question as to whether the specificity of the “grandmother neuron” could possibly be learned from unlabeled data. Informally, this would suggest that it is at least in principle possible that a baby learns to group faces into one class because it has seen many of them and not because it is guided by supervision or rewards.
</p></blockquote>
<p>After training, the authors use a test set of 37,000 images sampled from the &#8216;Labeled faces in the Wild&#8217; dataset and from ImageNet. These images are fed to the encoder, and the performance of each output neuron in classifying faces is measured&#8230;</p>
<blockquote><p>
  Surprisingly, the best neuron in the network performs very well in recognizing faces, despite the fact that no supervisory signals were given during training. The best neuron in the network achieves 81.7% accuracy in detecting faces.
</p></blockquote>
<p>It&#8217;s worth letting that sink in for a moment &#8211; just by repeatedly showing the network bunches of pixels, it has learned to encode a feature that represents the pattern of a face, without ever even knowing that there is such a thing as a face a priori.</p>
<p>Here are the 48 test images that most strongly stimulate the &#8216;face&#8217; neuron:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-3-top.jpeg?w=320" alt="" /></p>
<p>Definitely faces!</p>
<p>And somewhat creepily, here&#8217;s the ghost face that emerges when searching for the input that maximizes the firing of the neuron:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-3-bottom.jpeg?w=320" alt="" /></p>
<p>The neuron also turns out to be robust against &#8216;complex and difficult to hard-wire&#8217; invariances such as out-of-phase rotation and scaling.</p>
<blockquote><p>
  Having achieved a face-sensitive neuron, we would like to understand if the network is also able to detect other high-level concepts. For instance, cats and body parts are quite common in YouTube. Did the network also<br />
  learn these concepts?
</p></blockquote>
<p>A question of central importance to the Internet I&#8217;m sure you&#8217;ll agree, &#8216;is there a cat neuron, and if so what does the prototypical cat look like?&#8217; And yes, there is a learned cat neuron, and there is a human body neuron too! Although the maximal stimulation ghost images aren&#8217;t as impressive as the face one. The human body shape you can just about make out. The cat face I swear I could see at one point, but as of this time of writing it&#8217;s gone again! A bit like one of those optical illusions you have to stare at until you suddenly &#8216;see&#8217; it.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-6.jpeg?w=480" alt="" /></p>
<p>If you take a trained encoder and add a one-vs-all logistic classifier on top of the highest layer of the network, you have yourself a classifier&#8230;</p>
<h3>Generative adversarial nets (GANs)</h3>
<blockquote><p>
  So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties.
</p></blockquote>
<p>The core idea is simple to understand (the theoretical results showing why it works, a little less so!). Take a <em>generator</em> model G that generates (for example) images from noise. Pit it against a <em>discriminator</em> model D whose task it is to classify the image as either coming from the generator, or from the real data distribution.</p>
<blockquote><p>
  The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.
</p></blockquote>
<p>If the generative model and discriminative model are both multilayer perceptrons then both models can be trained using backpropagation and dropout. This special case is termed <em>adversarial nets</em>. We are searching for a solution where G recovers the training data distribution, and D is equal to 1/2 everywhere. Training alternates between <em>k</em> steps of optimizing D, and one step of optimizing G. &#8220;This results in D being maintained near its optimal solution, so long as G changes slowly enough.&#8221;</p>
<p>We want to learn the generator&#8217;s distribution <img src="https://s0.wp.com/latex.php?latex=p_g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p_g" title="p_g" class="latex" /> over data <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" />. Given input noise variables <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bz%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{z}" title="&#92;mathbf{z}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=G%28%5Cmathbf%7Bz%7D%2C%5Ctheta_g%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="G(&#92;mathbf{z},&#92;theta_g)" title="G(&#92;mathbf{z},&#92;theta_g)" class="latex" /> be a differential function representation by a multilayer perceptron, with parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta_g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;theta_g" title="&#92;theta_g" class="latex" />. Let <img src="https://s0.wp.com/latex.php?latex=D%28%5Cmathbf%7Bx%7D%2C%5Ctheta_d%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="D(&#92;mathbf{x},&#92;theta_d)" title="D(&#92;mathbf{x},&#92;theta_d)" class="latex" /> be a second multilayer perceptron outputting a single scalar representing the probability that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" /> came from the data rather than from <img src="https://s0.wp.com/latex.php?latex=p_g&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p_g" title="p_g" class="latex" />.</p>
<blockquote><p>
  We train D to maximize the probability of assigning the correct label to both training examples and samples from G. We simultaneously train G to minimize log(1 &#8211; D(G(z)).
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/gans-fig-1.jpeg?w=640" alt="" /></p>
<p>Below are examples of samples generated using this technique from the MNIST and TFD datasets. The rightmost column is the nearest training example of the neighbouring sample.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/gans-fig-2-top.jpeg?w=640" alt="" /></p>
<h3>Unsupervised representation learning with deep convolutional generative adversarial networks</h3>
<p>In the first two papers we looked at unsupervised learning of image features and at GANs. Now we get to put the two together&#8230;</p>
<blockquote><p>
  In this work, we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and show that they are a strong candidate for unsupervised learning.
</p></blockquote>
<p>If you recall the <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">amazing vector manipulations we did with word embeddings, such as King &#8211; Man + Woman = Queen</a>, then you&#8217;re in for a real treat when we get to do similar arithmetic with vectors representing visual concepts.</p>
<p>Here&#8217;s the core idea: we can train a GAN (unsupervised learning), which must somewhere internally encode representations useful for images, and then reuse parts of the generator and discriminator networks as feature extractors for <em>supervised</em> tasks.</p>
<p>There&#8217;s a small catch though, previous attempts to scale up GANs using CNNs met with limited success, often being unstable to train and resulting in generators that produce nonsensical outputs.</p>
<blockquote><p>
  &#8230; after extensive model exploration we identified a family of architectures that resulted in stable training across a range of datasets and allowed for training higher resolution and deeper generative models.
</p></blockquote>
<p>Here are the guidelines for training DCGANs:</p>
<ul>
<li>Replace any pooling layers with strided convolutions &#8211; this allows the network to learn its <em>own</em> spatial downsampling.</li>
<li>Remove any fully connected hidden layers on top of convolutional features. The authors found that connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well.</li>
<li>Use batch normalization, which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance.</li>
<li>Use ReLU activation for all generator layers, except for the final output layer where tanh works better</li>
<li>Use Leaky ReLU activation for all discriminator layers. (We can update that advice to PReLUs now).</li>
</ul>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-1.jpeg?w=640" alt="" /></p>
<p>DCGANs were trained on the Large-scene understanding (LSUN), ImageNet-1K and a Faces dataset. Here are examples of bedrooms generated after five epochs of training:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-3.jpeg?w=640" alt="" /></p>
<p>The discriminator network can then be used to build a supervised classifier:</p>
<blockquote><p>
  To evaluate the quality of the representations learned by DCGANs for supervised tasks, we train on Imagenet-1k and then use the discriminator’s convolutional features from all layers, maxpooling each layers representation to produce a 4 × 4 spatial grid. These features are then flattened and concatenated to form a 28672 dimensional vector and a regularized linear L2-SVM classifier is trained on top of them. This achieves 82.8% accuracy, out performing all K-means based approaches.
</p></blockquote>
<p>Section 6 of the paper is the really fun part though. This is where the authors set out to investigate what kinds of features the network has learned.</p>
<blockquote><p>
  If walking in the latent space results in semantic changes to the image generations (such as objects being added and removed), we can reason that the model has learned relevant and interesting representations&#8230;
</p></blockquote>
<p>And here a couple of compelling demonstrations that indicate this is indeed the case: a room without a window slowing being transformed into a room with a large window, and a TV slowly being transformed into a window.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-bedroom-transform-1.jpeg?w=480" alt="" /><br />
<img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-bedroom-transform-2.jpeg?w=480" alt="" /></p>
<blockquote><p>
  We demonstrate that an unsupervised DCGAN trained on a large image dataset can also learn a hierarchy of features that are interesting. Using guided backpropagation as proposed by (Springenberg et al., 2014), we show in Fig.5 that the features learnt by the discriminator activate on typical parts of a bedroom, like beds and windows.
</p></blockquote>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-5r.jpeg?w=320" alt="" /></p>
<p>To see whether the generator learns specific object representations (as we saw in our first paper today) , the authors perform a similar analysis to find the neurons with the strongest activation in the presence of windows. Then random new samples are generated with (top row) and without (bottom row) these features included.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-6.jpeg?w=640" alt="" /></p>
<p>And now the moment you&#8217;ve been waiting for&#8230;</p>
<blockquote><p>
  In the context of evaluating learned representations of words (Mikolov et al., 2013) demonstrated that simple arithmetic operations revealed rich linear structure in representation space. One canonical example demonstrated that the vector(”King”) &#8211; vector(”Man”) + vector(”Woman”) resulted in a vector whose nearest neighbor was the vector for Queen. We investigated whether similar structure emerges in the Z representation of our generators. We performed similar arithmetic on the Z vectors of sets of exemplar samples for visual concepts. Experiments working on only single samples per concept were unstable, but averaging the Z vector for three examplars showed consistent and stable generations that semantically obeyed the arithmetic.
</p></blockquote>
<p>In the picture below, the three top pictures in each column (Z vectors) were averaged to produce the picture (Z vector) you see below them, and then the vector operations were applied. The result is the center image in the box of nine that you see on the right-hand side, the other images surrounding it have small amounts of noise added.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-7.jpeg?w=640" alt="" /></p>
<p>And if you&#8217;ll indulge me one more time, here&#8217;s the extraction of a &#8216;turn&#8217; vector by differencing left-facing and right-facing faces. When you apply it to new face images, you can change the pose!</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-8.jpeg?w=640" alt="" /></p>
<h3>Improved techniques for training GANs</h3>
<p>This paper focuses on the use of GANs for semi-supervised learning and image generation, but along the way (as the title suggests), it makes key contributions in two areas: how to efficiently train GANs, and how to evaluate the quality of generated images.</p>
<blockquote><p>
  &#8230; training GANs requires finding a Nash equilibrium of a non-convex game with continuous, high-dimensional parameters. GANs are typically trained using gradient descent techniques that are designed to find a low value of a cost function, rather than to find the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge.
</p></blockquote>
<p>There are five heuristics for improving training.</p>
<ol>
<li>Instead of directly maximising the output of the generator, require the generator to generate data matching the <em>statistics</em> of the real data. &#8220;<em>Specifically, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to find those features that are most discriminative of real data versus data generated by the current model.</em>&#8221; This process is called <strong>feature matching</strong>.</li>
<li>Use <strong>mini-batching</strong> to look at multiple data examples in combination. This help to prevent the generator collapsing to a mode where it always emits the same point.</li>
<li>Use <strong>historical averaging</strong> by adding a term to each player&#8217;s cost function that looks at the difference between the current parameter values and those over the last <em>t</em> time steps. &#8216;<em>This approach is loosely inspired by the fictitious play algorithm that can find equilibria in other kinds of games.</em>&#8216;</li>
<li>Use <strong>one-side label smoothing</strong>. This is the idea of replacing 0 and 1 targets for a classifier with smoothed values, as we saw with <a href="https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/">distillation</a>. Best results are obtained when smoothing only <em>positive</em> labels, leaving negative labels set to zero (hence &#8216;one-sided&#8217;).</li>
<li>Use <strong>virtual batch normalization</strong>. Chose a fixed reference set of examples at the start of training, and normalize this set using its own statistics. Then during training each example <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" /> is normalized based on the statistics from this reference batch.</li>
</ol>
<p>Now we turn our attention to the question of how to figure out whether the generated images are any good.</p>
<blockquote><p>
  Generative adversarial networks lack an objective function, which makes it difficult to compare performance of different models.
</p></blockquote>
<p>One option is to use human judges (e.g. via Amazon Mechanical Turk) &#8211; this is comparatively slow though, and its hard to get consistent ratings. Seeking an automatic method to evaluate samples, the authors apply the <a href="https://arxiv.org/pdf/1512.00567.pdf">Inception model</a> to every generated image to get the conditional label distribution <img src="https://s0.wp.com/latex.php?latex=p%28y%7C%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p(y|&#92;mathbf{x})" title="p(y|&#92;mathbf{x})" class="latex" /> . If the generated image contains meaningful objects, we should expect the label distribution to have low entropy (i.e., to show strong preference for one or a small number of class labels because it is something that the Inception network recognises). And since we also want the model to generate <em>varied</em> images, then looking at the label distributions across a whole set of images should show high entropy (i.e., the generated images contain a variety of different objects). These two requirements are combined to create the <em>Inception Score</em> metric.</p>
<p>Now that we can train GANs efficiently, and we know how to evaluate the generator, we can use GAN generators during semi-supervised learning. The generated images are used to extend the training dataset (e.g. 50% real images, and 50% generated). It works like this:</p>
<ul>
<li>Take any classifier, making predictions across <em>K</em> classes.</li>
<li>Increase the dimension of the classifier output by one, to make <em>K+1</em> dimensions, this new <code>generated</code> class will represent generated images.</li>
<li><img src="https://s0.wp.com/latex.php?latex=p_%7Bmodel%7D%28y+%3D+K%2B1+%7C+%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="p_{model}(y = K+1 | &#92;mathbf{x})" title="p_{model}(y = K+1 | &#92;mathbf{x})" class="latex" /> is therefore the probability assigned to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;mathbf{x}" title="&#92;mathbf{x}" class="latex" /> being a generated image. This corresponds to <img src="https://s0.wp.com/latex.php?latex=1+-+D%28%5Cmathbf%7Bx%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="1 - D(&#92;mathbf{x})" title="1 - D(&#92;mathbf{x})" class="latex" /> in the original GAN framework.</li>
<li>We can now learn from both <em>labeled</em> and <em>unlabeled</em> data. Given unlabeled data we can use it for classifier training so long as the predicted class is not &#8216;<code>generated</code>.&#8217;</li>
<li>The loss function for training the classifier becomes <img src="https://s0.wp.com/latex.php?latex=L_%7Bsupervised%7D+%2B+L_%7Bunsupervised%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="L_{supervised} + L_{unsupervised}" title="L_{supervised} + L_{unsupervised}" class="latex" />.</li>
</ul>
<blockquote><p>
  In practice, L<sub>unsupervised</sub> will only help if it is not trivial to minimize for our classifier and we thus need to train G to approximate the data distribution. One way to do this is by training G to minimize the GAN game-value, using the discriminator D defined by our classifier. This approach introduces an interaction between G and our classifier that we do not fully understand yet, but empirically we find that optimizing G using feature matching GAN works very well for semi-supervised learning, while training G using GAN with minibatch discrimination does not work at all.
</p></blockquote>
<p>When used purely for image generation though, minibatch discrimination yields better results, as can be seen in these generated MNIST digits:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/improved-gan-fig-3.jpeg?w=480" alt="" /></p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4059&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/03/02/unsupervised-learning-and-gans/feed/</wfw:commentRss>
            <slash:comments>2</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-3-top.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-3-bottom.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/unsupervised-hl-features-fig-6.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/gans-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/gans-fig-2-top.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-3.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-bedroom-transform-1.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-bedroom-transform-2.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-5r.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-6.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-7.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dcgan-fig-8.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/improved-gan-fig-3.jpeg?w=480" medium="image" />
        </item>
        <item>
            <title>Optimisation and training techniques for deep learning</title>
            <link>https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/</link>
            <comments>https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/#respond</comments>
            <pubDate>Wed, 01 Mar 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4057</guid>
            <description><![CDATA[Today we&#8217;re looking at the &#8216;optimisation and training techniques&#8217; section from the &#8216;top 100 awesome deep learning papers&#8217; list. Random search for hyper-parameter optimization, Bergstra &#38; Bengio 2012 Improving neural networks by preventing co-adaptation of feature detectors, Hinton et al., 2012 Dropout: a simple way to prevent neural networks from overfitting, Srivastava et al., 2014 [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4057&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>Today we&#8217;re looking at the &#8216;optimisation and training techniques&#8217; section from the <a href="https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques">&#8216;top 100 awesome deep learning papers&#8217; list</a>.</p>
<ul>
<li><a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a">Random search for hyper-parameter optimization</a>, Bergstra &amp; Bengio 2012</li>
<li><a href="http://arxiv.org/pdf/1207.0580.pdf">Improving neural networks by preventing co-adaptation of feature detectors</a>, Hinton et al., 2012</li>
<li><a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">Dropout: a simple way to prevent neural networks from overfitting</a>, Srivastava et al., 2014</li>
<li><a href="http://arxiv.org/pdf/1412.6980">Adam: a method for stochastic optimization</a>, Kingma &amp; Ba, 2015</li>
<li><a href="http://arxiv.org/pdf/1502.03167">Batch normalization: accelerating deep network training by reducing internal covariate shift</a>, Ioffy &amp; Szegedy, 2015</li>
<li><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</a>, He et al., 2015</li>
</ul>
<h3>Random search for hyper-parameter optimization</h3>
<p>A machine learning model is itself parameterised by a large number of different parameters (e.g., learning rate, number of hidden units, strength of weight regularization). How you set these <em>hyper-parameters</em> can have a big impact on the overall results achieved, but finding an optimal set of hyper-parameters is far from easy. Essentially it boils down to picking some sets of parameters and trying them to see how well they work. How do you choose which sets to pick though? Even with a relatively small number of parameters it&#8217;s impossible to do an exhaustive search as the search space grows exponentially with the number of hyper-parameters. Furthermore, each trial is expensive &#8211; you have to train the full model using the hyper-parameters and see how well it performs, so practically you can&#8217;t do too many of them. In fact, it&#8217;s plain to see that you can only cover a tiny fraction of the overall search space.</p>
<p>Two common approaches for finding good hyper-parameter values as of 2012 were human intuition guided search, and a grid search in which you explore points in an evenly spaced grid laid over the search space.</p>
<blockquote><p>
  A major drawback of manual search is the difficulty in <em>reproducing results</em>. This is important both for the progress of scientific research in machine learning as well as for ease of application of learning algorithms by non-expert users. On the other hand, grid search along does very poorly in practice (as discussed here).
</p></blockquote>
<p>What the authors show, is that a <em>random search</em> turns out to be a surprisingly effective technique, while also being just as easy to implement as a parallel search. The reason random search turns out to work so well is due to two key properties:</p>
<ol>
<li>It turns out that the parameter space has a <em>low effective dimensionality</em> . That is, some parameters matter much more than others when it comes to finding good settings.</li>
<li>Which parameters those are varies according to the dataset (i.e., you can&#8217;t just find the two most important parameters for some model architecture and then always optimise based on just those).</li>
</ol>
<p><em>If</em> a researcher could know ahead of time which subspaces are important, then they could design an appropriate grid. But when they can&#8217;t know this, grid search fails. Consider the following simple illustration in 2-dimensional space. We have one parameter that turns out to be important (on the x-axis) and one that turns out to be unimportant (on the y-axis).</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/random-search-fig-1.jpeg?w=640" alt="" /></p>
<p>The grid search only touches three points along the important dimension, whereas the random search tests nine points &#8211; thus the random search has a much better chance of finding a good setting along this dimension.</p>
<p>To study the effects with real networks the authors plot <em>random experiment efficiency curves</em>, which look like this:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/random-search-fig-2.jpeg?w=480" alt="" /></p>
<p>At each point <em>x</em> on the x-axis, you are seeing the results of exploring multiple hyper-parameter optimisation experiments, where each experiment trialled <em>x</em> different hyper-parameter sets. Thus on the y-axis we see the accuracy you could expect to achieve if running a hyper-parameter search with <em>x</em> trials.</p>
<blockquote><p>
  There are two general trends in random experiment efficiency curves, such as the one in Figure 2: a sharp upward slope of the lower extremes as experiments grow, and a gentle downward slope of the upper extremes. The sharp upward slope occurs because when we take the maximum over larger subsets of the S trials, trials with poor performance are rarely the best within their subset. It is natural that larger experiments find trials with better scores. The shape of this curve indicates the frequency of good models under random search, and quantifies the relative volumes (in search space) of the various levels of performance.
</p></blockquote>
<p>Now that we know how to interpret those curves, here are the results of neural network hyper-parameter optimisation experiments using random search, as compared to grid search (the blue line):</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/random-search-fig-6.jpeg?w=640" alt="" /></p>
<p>And this plot reinforces the point that only a small number of parameters are significant in each case, but the relative important of each individual hyper-parameter varies from one dataset to the next:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/random-search-fig-7.jpeg?w=640" alt="" /></p>
<p>Deep networks have way more hyper-parameters, and we&#8217;d expect manual search to have more impact here. Experiments with a Deep Belief Network (DBN) with 32 hyper-parameters across 8 different datasets showed that random search found a better model than manual search for one dataset, an equally good model in four, and an inferior model in three.</p>
<blockquote><p>
  In this more challenging optimization problem, random search is still effective, but not superior as it was in the case of neural network optimisation.
</p></blockquote>
<p>There is a huge body of work on global optimisation, much of which could be applied to hyper-parameter optimisation. However, grid search and random search have the advantage of being very simple to understand and implement.</p>
<blockquote><p>
  With so many sophisticated algorithms to draw on, it may seem strange that grid search is still widely used, and, with straight faces, we now suggest using random search instead. We believe the reason for this state of affairs is a technical one. Manual optimization followed by grid search is easy to implement: grid search requires very little code infrastructure beyond access to a cluster of computers. Random search is just as simple to carry out, uses the same tools, and fits in the same workflow. Adaptive search algorithms on the other hand require more code complexity. They require client-server architectures in which a master process keeps track of the trials that have completed, the trials that are in progress, the trials that were started but failed to complete.
</p></blockquote>
<h3>Improving neural networks by preventing co-adaptation of feature detectors</h3>
<p>This short paper presents a very simple technique to reduce overfitting when training networks. For each training example presented to the network, each hidden unit is randomly omitted with probability 0.5. We call this <em>dropout</em>. Dropout prevents complex <em>co-adaptations</em> of feature detectors since a given hidden unit cannot rely on other hidden units being present. This ensures each neuron learns to make an independently useful contribution.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dropout-sketch.jpeg?w=640" alt="" /></p>
<blockquote><p>
  Another way to view the dropout procedure is as a very efficient way of performing model averaging with neural networks. A good way to reduce the error on the test set is to average the predictions produced by a very large number of different networks. The standard way to do this is to train many separate networks and then to apply each of these networks to the test data, but this is computationally expensive during both training and testing. Random dropout makes it possible to train a huge number of different networks in a reasonable time. There is almost certainly a different network for each presentation of each training case but all of these networks share the same weights for the hidden units that are present.
</p></blockquote>
<p>Here&#8217;s an example of dropout at work, where you can see it makes a big difference to the test error.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/preventing-co-adaptation-fig-2.jpeg?w=640" alt="" /></p>
<p>After some experimentation, a dropout probability of 50% for hidden layers and 20% for input units seems to work well.</p>
<h3>Dropout: a simple way to prevent neural networks from overfitting</h3>
<p>This is a longer paper from many of the same authors exploring the same fundamental idea as above. We get a much more comprehensive set of evaluations, which show that dropout can improve neural net performance in a wide variety of application domains including object classification, digit recognition, speech recognition, document classification, and analysis of computational biology data. &#8220;<em>This suggests that dropout is a general technique and is not specific to any domain.</em>&#8221; The idea can also be extended to other models, and the authors show an application with Restricted Boltzman Machines.</p>
<p>We also get a slightly deeper explanation of what&#8217;s going on when dropout is used, and how to test using a model trained using dropout:</p>
<blockquote><p>
  Applying dropout to a neural network amounts to sampling a &#8220;thinned&#8221; network from it. The thinned network consists of all the units that survived dropout. A neural net with n units, can be seen as a collection of 2<sup>n</sup> possible thinned neural networks. These networks all share weights so that the total number of parameters is still O(n<sup>2</sup>), or less. For each presentation of each training case, a new thinned network is sampled and trained. So training a neural network with dropout can be seen as training a collection of 2<sup>n</sup> thinned networks with extensive weight sharing, where each thinned network gets trained very rarely, if at all.
</p></blockquote>
<p>At test time, you use the network as a single neural net, but with weights that are <em>scaled-down</em> versions of the trained weights. If a unit is retained with probability <em>p</em> during training, the outgoing weights of that unit are multiplied by <em>p</em> when testing.</p>
<p>A form of regularisation called <em>max-norm</em> regularisation was found to work well in conjunction with dropout. This simply constrains the norm of the weight vector at each hidden unit to be (upper) bounded by a fixed constant <em>c</em> (clipping). If <strong>w</strong> is the vector of weights, then we want <img src="https://s0.wp.com/latex.php?latex=%7C%7C%5Cmathbf%7Bw%7D%7C%7C_%7B2%7D+%5Cleq+c&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="||&#92;mathbf{w}||_{2} &#92;leq c" title="||&#92;mathbf{w}||_{2} &#92;leq c" class="latex" />.</p>
<blockquote><p>
  Although dropout alone gives significant improvements, using dropout along with max-norm regularization, large decaying learning rates and high momentum provides a significant boost over just using dropout.
</p></blockquote>
<p>Dropout does have one disadvantage: it increases training by a factor of 2-3. Thus you can trade off between overfitting and training time by tweaking the dropout rate.</p>
<p>Appendix A contains some useful hints for training dropout networks in practice.</p>
<h3>Adam: a method for stochastic optimization</h3>
<blockquote><p>
  Stochastic gradient-based optimization is of core practical importance in many fields of science and engineering&#8230; Stochastic gradient descent proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning.
</p></blockquote>
<p><em>Adam</em> is a stochastic optimisation technique for high-dimensional parameter spaces and noisy objectives (such as the <em>noise introduced by using dropouts</em>). It has per-parameter adaptive learning rates, and combines the advantages of two recent methods you may have seen popping up in papers: AdaGrad and RMSProp. It also works well in online settings, is straightforward to implement, and requires little memory.</p>
<blockquote><p>
  Some of Adam&#8217;s advantages are that the magnitudes of parameter updates are invariant to rescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyper-parameter, it does not require a stationary objective, it works well with sparse gradients, and it naturally performs a form of step size annealing.
</p></blockquote>
<p>When used in logistic regression, Adam gives similar convergence as SGD with momentum (Nesterov), and faster convergence than Adagrad:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/adam-fig-1.jpeg?w=640" alt="" /></p>
<p>Fast convergence is also demonstrated with MNIST multi-layer neural networks :</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/adam-fig-2.jpeg?w=600" alt="" /></p>
<p>And finally, Adam gives very impressive results when used with convolutional neural networks.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/adam-fig-3.jpeg?w=600" alt="" /></p>
<blockquote><p>
  Overall, we found Adam to be robust and well-suited to a wide range of non-convex optimization problems in the field of machine learning.
</p></blockquote>
<p>The pseudo-code for Adam is shown below:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/adam-alg-1.jpeg?w=640" alt="" /></p>
<blockquote><p>
  The algorithm updates exponential moving averages of the gradient (<em>m<sub>t</sub></em>) and the squared gradient (<em>v<sub>t</sub></em>) where the hyper-parameters <em>β1, β2 ∈ [0, 1)</em> control the exponential decay rates of these moving averages. The moving averages themselves are estimates of the 1st moment (the mean) and the 2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are initialized as (vectors of) 0’s, leading to moment estimates that are biased towards zero, especially during the initial timesteps, and especially when the decay rates are small (i.e. the βs are close to 1). The good news is that this initialization bias can be easily counteracted, resulting in bias-corrected estimates <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bm%7D_t&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{m}_t" title="&#92;hat{m}_t" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bv%7D_t&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{v}_t" title="&#92;hat{v}_t" class="latex" />.
</p></blockquote>
<p>Initialization bias correction is discussed in section 3 of the paper, the short version is that you divide by <img src="https://s0.wp.com/latex.php?latex=%281+-+%5Cbeta_%7B1%7D%5E%7Bt%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(1 - &#92;beta_{1}^{t})" title="(1 - &#92;beta_{1}^{t})" class="latex" /> to correct the first moment estimate, and by <img src="https://s0.wp.com/latex.php?latex=%281+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%29&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="(1 - &#92;beta_{2}^{t})" title="(1 - &#92;beta_{2}^{t})" class="latex" /> to correct the second moment estimate (as can be seen in the algorithm pseudocode).</p>
<p>The hyper-parameter α sets an upper bound on the magnitude of steps in parameter space, and therefore <em>&#8220;we can often deduce the right order of magnitude of α such that optima can be reached from θ<sub>0</sub> within some number of iterations.&#8221;</em></p>
<p>Sebastian Ruder has a very helpful blog post entitled &#8220;<a href="http://sebastianruder.com/optimizing-gradient-descent/">An overview of gradient descent optimisation algorithms</a>&#8221; that starts with vanilla SGD, shows some of the issues with it, and introduces a series of algorithms leading up to Adam that gradually chip away at those issues. There&#8217;s a nice animated visualisation too.</p>
<h3>Batch normalization: accelerating deep network training by reducing internal covariate shift</h3>
<p>Network training converges faster if inputs to the network are <em>whitened</em>. That is, inputs are linearly transformed to have zero means and unit variances. But consider a multi-layer network, by the time we&#8217;re a few layers in, the inputs to layer <em>n</em> are now some combinations of the outputs of layer <em>n-1</em>, and as training progresses we lose the normalisation effect. This brings back some of the problems than normalisation/whitening are intended to reduce: vanishing gradients, slow training, and saturation.</p>
<p>The fancy name for this change in the distributions of the internal nodes of a deep network over the course of time during training is <em>Internal Covariate Shift</em>. The central idea of batch normalisation is to bring back the benefits of normalisation not just for the initial inputs, but at every layer of the network. In theory this would give us faster convergence, and in practice it seems to be much faster:</p>
<blockquote><p>
  &#8230; we apply Batch Normalization to the best-performing ImageNet classification network, and show that we can match its performance using only 7% of the training steps&#8230;
</p></blockquote>
<p>(I&#8217;ll just pause there for a moment to let that sink in)</p>
<blockquote><p>
  &#8230; and can further exceed its accuracy by a substantial margin. Using an ensemble of such networks trained with Batch Normalization, we achieve a top-5 error rate that improves upon the best known results on ImageNet classification.
</p></blockquote>
<p>In batch normalisation, each scalar feature in a layer is independently normalised by making it have a mean of zero and variance of 1. For the k<sup>th</sup> dimension:<br />
<img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D%5E%7B%28k%29%7D+%3D+%5Cfrac%7Bx%5E%7B%28k%29%7D+-+E%5Bx%5E%7B%28k%29%7D%5D%7D%7B%5Csqrt%7BVar%5Bx%5E%7B%28k%29%7D%5D%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;hat{x}^{(k)} = &#92;frac{x^{(k)} - E[x^{(k)}]}{&#92;sqrt{Var[x^{(k)}]}}" title="&#92;hat{x}^{(k)} = &#92;frac{x^{(k)} - E[x^{(k)}]}{&#92;sqrt{Var[x^{(k)}]}}" class="latex" /></p>
<p>where the expectation and variance are computed over the training set data. Assuming mini-batch based stochastic gradient descent training, then in each mini-batch we will produce estimates of the mean and variance of each activation. &#8220;<em>This way, the statistics used for normalization can fully participate in the gradient backpropagation.</em>&#8221;</p>
<p>The full <em>Batch Normalization Transform</em> is as follows, where &amp;eps; is a constant added to the mini-batch variance for numerical stability:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/batch-normalisation-alg-1.jpeg?w=480" alt="" /></p>
<p>The parameters γ and β are used to scale and shift the normalized value, so that normalisation does not end up constraining what the layer can represent.</p>
<blockquote><p>
  To <em>Batch-Normalize</em> a network, we specify a subset of activations and insert the BN transform for each of them, according to Alg. 1. Any layer that previously received x as the input, now receives BN(x). A model employing Batch Normalization can be trained using batch gradient descent, or Stochastic Gradient Descent with a mini-batch size m \&gt; 1, or with any of its variants such as Adagrad [or Adam].
</p></blockquote>
<p>When using batch normalisation, the following changes are also recommended:</p>
<ul>
<li>increasing the learning rate (batch normalisation allows faster learning with no adverse effects)</li>
<li><em>removing</em> dropout (batch normalisation achieves some of the same goals, and avoids overfitting)</li>
<li>reducing L<sub>2</sub> weight regularization (by a factor of 5 in the author&#8217;s tests)</li>
<li>accelerating the learning rate decay (6x faster)</li>
<li>removing local response normalisation if you are using it</li>
<li>shuffle training examples more thoroughly</li>
<li>use less distortion of images in training sets, to let the trainer focus more on &#8216;real&#8217; images</li>
</ul>
<h3>Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</h3>
<blockquote><p>
  &#8230; the rectifier neuron, Rectified Linear Unit (ReLU) is one of several keys to the recent success of deep networks. It expedites convergence of the training procedure and leads to better solutions than conventional sigmoid-like units. Despite the prevalence of rectifier networks, recent improvements of models and theoretical guidelines for training them have rarely focused on the properties of the rectifiers.
</p></blockquote>
<p>In other words, everybody is just using the standard ReLU that looks like this:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/delving-deep-relu.jpeg?w=320" alt="" /></p>
<p>The authors make two key advances in this paper. Firstly they show a modified version of the standard ReLU, called a <em>Parametric Rectified Linear Unit</em> (PReLu) which can adaptively learn the parameters of the rectifiers, improving accuracy at negligible extra computing cost. Secondly they introduce a new method for initialising parameters which helps with the convergence of very deep models trained directly from scratch.</p>
<p>Do these enhancements make a big difference? Yes!</p>
<blockquote><p>
  Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%,) on this dataset.
</p></blockquote>
<p>PReLus have a learned parameter <em>a</em> which controls the slope of the negative part:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/delving-deep-prelu.jpeg?w=480" alt="" /></p>
<p>(Note that when <em>a</em> is small and fixed, PReLu becomes Leaky ReLU).</p>
<p>PreLU is trained using backpropagation and optimized simultaneously with other layers.</p>
<blockquote><p>
  Rectifier networks are easier to train than tradititional sigmoid-like activation networks, but a bad initialization can still hamper the learning of a highly non-linear system&#8230;
</p></blockquote>
<p>Deep CNNs are typically initialised with random weights drawn from Gaussian distribution, but this can lead to situations where the magnitudes of input signals are reduced or magnified exponentially. Instead, it is better to draw initial weights from a zero-mean Gaussian distribution with standard deviation <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%2Fn_l%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sqrt{2/n_l}" title="&#92;sqrt{2/n_l}" class="latex" /> for forward propagation, and <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%2F%5Chat%7Bn%7D_l%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="&#92;sqrt{2/&#92;hat{n}_l}" title="&#92;sqrt{2/&#92;hat{n}_l}" class="latex" /> for backpropagation (<img src="https://s0.wp.com/latex.php?latex=n_l&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="n_l" title="n_l" class="latex" /> is the number of connections in the layer). Doing this avoids the exponential problem. See §2.2 for the full gory details.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/delving-deep-fig-4.jpeg?w=640" alt="" /></p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4057&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/feed/</wfw:commentRss>
            <slash:comments>0</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/random-search-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/random-search-fig-2.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/random-search-fig-6.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/random-search-fig-7.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dropout-sketch.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/preventing-co-adaptation-fig-2.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/adam-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/adam-fig-2.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/adam-fig-3.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/adam-alg-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/batch-normalisation-alg-1.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/delving-deep-relu.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/delving-deep-prelu.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/delving-deep-fig-4.jpeg?w=640" medium="image" />
        </item>
        <item>
            <title>When DNNs go wrong &#8211; adversarial examples and what we can learn from them</title>
            <link>https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/</link>
            <comments>https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/#comments</comments>
            <pubDate>Tue, 28 Feb 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>
            <category><![CDATA[Security]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4055</guid>
            <description><![CDATA[Yesterday we looked at a series of papers on DNN understanding, generalisation, and transfer learning. One additional way of understanding what&#8217;s going on inside a network is to understand what can break it. Adversarial examples are deliberately constructed inputs which cause a network to produce the wrong outputs (e.g., misclassify an input image). We&#8217;ll start [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4055&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>Yesterday we looked at a series of papers on DNN understanding, generalisation, and transfer learning. One additional way of understanding what&#8217;s going on inside a network is to understand what can <em>break</em> it. <em>Adversarial examples</em> are deliberately constructed inputs which cause a network to produce the wrong outputs (e.g., misclassify an input image). We&#8217;ll start by looking at &#8216;Deep Neural Networks are Easily Fooled&#8217; from the &#8216;<a href="https://github.com/terryum/awesome-deep-learning-papers#optimization--training-techniques">top 100 awesome deep learning papers list</a>,&#8217; and then move on to some other examples cited in the excellent recent OpenAI post on &#8220;<a href="https://openai.com/blog/adversarial-example-research/">Attacking machine learning with adversarial examples</a>.&#8221;</p>
<p>The papers we&#8217;ll be covering today are therefore:</p>
<ul>
<li><a href="http://arxiv.org/pdf/1412.1897">Deep neural networks are easily fooled</a>, Nguyen et al., 2015</li>
<li><a href="https://arxiv.org/pdf/1602.02697.pdf">Practical black-box attacks against deep learning systems using adversarial examples</a>, Papernot et al., 2016</li>
<li><a href="https://arxiv.org/pdf/1607.02533.pdf">Adversarial examples in the physical world</a>, Goodfellow et al., 2017</li>
<li><a href="https://arxiv.org/pdf/1412.6572.pdf">Explaining and harnessing adversarial examples</a>, Goodfellow et al., 2015</li>
<li><a href="https://arxiv.org/pdf/1511.04508.pdf">Distillation as a defense to adversarial perturbations against deep neural networks</a>, Papernot et al., 2016</li>
<li><a href="https://arxiv.org/pdf/1701.04143.pdf">Vulnerability of deep reinforcement learning to policy induction attacks</a>, Behzadan &amp; Munir, 2017</li>
<li><a href="http://rll.berkeley.edu/adversarial/">Adversarial attacks on neural network policies</a>, Huang et al. 2017</li>
</ul>
<p>Might I suggest <em>two</em> cups of coffee for this one&#8230;</p>
<h3>Deep neural networks are easily fooled</h3>
<p>What&#8217;s this?</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-armadillo.jpeg?w=200" alt="" /></p>
<p><em>Clearly</em> it&#8217;s an armadillo! I&#8217;ll make it easier for you&#8230; these are five different images of a digit between 0 and 9, but which one?</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fours.jpeg?w=100" alt="" /></p>
<p>(It&#8217;s a 4, obviously).</p>
<p>What you&#8217;re seeing here are <em>adversarial</em> images, deliberately crafted to classify as some class <em>x</em>, while clearly looking nothing like the target class from a human perspective.</p>
<blockquote><p>
  &#8230; it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g., labelling with certainty that white noise static is a lion).
</p></blockquote>
<p>The fact that we can do this tells us something about interesting about the differences between DNN vision and human vision. Clearly, the DNNs are not learning to interpret images in the same way that we do.</p>
<p>The adversarial images are created using an evolutionary algorithm (EA) that evolves a population of images. Standard EAs use a single fitness function, but the authors here use a new algorithm called MAP-Elites that allows simultaneous evolution of a population containing individuals scoring well on many classes &#8211; in each round the best individual so far for each objective is kept. Two different mutation strategies are tested: one that directly encodes pixels in grayscale and then mutates their values, and one that uses an <em>indirect encoding</em> based on a compositional pattern-producing network (CPNN) which can evolve complex regular images that resemble natural and man-made objects.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-2.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-2.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<p>Take MNIST as an example (digits 0-9). Starting with clean images, within 50 generations images are produces that MNIST DNNs will misclassify with 99.99% confidence but are unrecognisable as such. These images were created used the direct encoding mutation:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-4.jpeg?w=480" alt="" /></p>
<p>And these were created using the indirect encoding mutation:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-5.jpeg?w=480" alt="" /></p>
<p>Using the CPNN encoding and deliberately evolving images to match target DNN classes results in a wide variety of images:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-8.jpeg?w=640" alt="" /></p>
<blockquote><p>
  For many of the produced images, one can begin to identify why the DNN believes the image is of that class once given the class label. This is because evolution need only produce features that are unique to, or <em>discriminative</em> for, a class, rather than produce an image that contains all of the typical features of a class.
</p></blockquote>
<p>By removing some of the repeated elements from the generated images, the confidence score of the DNN drops. &#8220;<em>These results suggest that DNNs tend to learn low and middle-level features rather than the global structure of objects.</em>&#8221;</p>
<p>You might wonder if we can make a DNN more robust to such adversarial images by extending the training regime to include such negative examples. The authors tried this, but found that it was always possible to generate new adversarial examples that still fooled the resulting network (this remained true even after 15 iterations of the process).</p>
<p>Why is it so easy to generate adversarial examples? Discriminative models create decision boundaries that partition data into classification regions. In a <em>high-dimensional</em> input space, the area a model allocates to a class may be much larger than the area occupied by training examples for the class. This leaves plenty of room for adversarial images&#8230;</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-14.jpeg?w=480" alt="" /></p>
<p>We now turn our attention from adversarial examples as a way of <em>understanding</em> what DNNs are doing, to adversarial examples as a way of <em>attacking</em> DNNs..</p>
<blockquote><p>
  The fact that DNNs are increasingly used in a wide variety of industries, including safety-critical ones such as driverless cars, raises the possibility of costly exploits via techniques that generate fooling images&#8230;
</p></blockquote>
<h3>Practical black-box attacks against deep learning systems using adversarial examples</h3>
<p>This is a panda (59.7% confidence):</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-panda.jpeg?w=320" alt="" /></p>
<p>But <em>this</em> is obviously a gibbon (99.3% confidence):</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-gibbon.jpeg?w=320" alt="" /></p>
<p>(From &#8216;Explaining and harnessing adversarial examples,&#8217; which we&#8217;ll get to shortly).</p>
<p>The goal of an attacker is to find a small, often imperceptible perturbation to an existing image to force a learned classifier to misclassify it, while the same image is still correctly classified by a human. Previous techniques for generating adversarial images relied on either access to the full training set, and/or the hidden weights in the network. What this paper shows is that successful attacks can be mounted even without such information &#8211; all you need is the ability to pass an input to the classifier, and learn the resulting predicted class.</p>
<blockquote><p>
  Our threat model thus corresponds to the real-world scenario of users interacting with classifiers hosted remotely by a third-party keeping the model internals secret. In fact, we instantiate our attack against classifiers served by MetaMind, Amazon, and Google. Models are automatically trained by the hosting platform. We are capable of making labeling prediction queries only after training is completed. Thus, we provide the first correctly blinded experiments concerning adversarial examples as a security risk.
</p></blockquote>
<p>The attack works by training a substitute model (owned by the attacker) using the target DNN as an oracle. Target inputs are synthetically generated, passed to the oracle (system under attack), and the output labels becomes the training labels for the substitute model. One the substitute DNN has been trained, adversarial images can be created that succeed against the substitute DNN, using normal white box techniques.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/practical-black-box-fig-3.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/practical-black-box-fig-3.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<p>Crucially, <em>the images that fool the substitute network also turn out to often force the same misclassifications in the target model</em>. Since the attacker only needs to (presumably) find one such image that transfers successfully this should be possible with high likelihood. It doesn&#8217;t even matter if the substitute DNN has a different architecture to the target model (which it likely will, because we assume the attacker does not know the target architecture) &#8211; so long as the substitute DNN is appropriate to the kind of classification task (e.g. CNN for image classification) the attack works well. In fact, the attack doesn&#8217;t only work with DNN targets &#8211; it generalizes to additional machine learning models (tested with logistic regression, SVMs, decision trees, and nearest neighbours).</p>
<p>The authors showed the ability to attack networks blind by using three cloud ML services provide by MetaMind, Google, and Amazon respectively. In each case training data is uploaded to the service, which learns a classifier (the user has no idea what model the service uses for this). Then the substitute network technique is used to find examples that fool the learned classifier.</p>
<blockquote><p>
  An adversary using our attack model can reliably force the DNN trained using MetaMind on MNIST to misclassify 82.84% of adversarial examples crafted with a perturbation not affecting human recognition.
</p></blockquote>
<p>An Amazon classifier that achieved 92.17% test accuracy on MNIST could be fooled by 96.19% of adversarial examples. The Google classifier achieved 92% test accuracy on MNIST and could be fooled by 88.94% of adversarial examples. Defences based on gradient masking are <em>not effective</em> against the substitute attack.</p>
<h3>Adversarial examples in the physical world</h3>
<p>So now we know that you don&#8217;t need access to a model in order to successfully attack it. But there&#8217;s more&#8230;</p>
<blockquote><p>
  Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
</p></blockquote>
<p>The authors print clean and adversarial images, take photos of the printed images, crop those photos to be the same size as the originals, and then pass these into the classifier. The procedure takes place with manual photography and no careful control of lighting, camera angle etc., thus in introduces nuisance variability with the potential to destroy adversarial perturbations depending on subtle changes.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/adversarial-physical-fig-3.jpeg?w=640" alt="" /></p>
<blockquote><p>
  Overall, the results show that some fraction of adversarial examples stays misclassified even after a non-trivial transformation: the photo transformation. This demonstrates the possibility of physical adversarial examples. For example, an adversary using the fast method with ε = 16 could expect that about 2/3 of the images would be top-1 misclassified and about 1/3 of the images would be top-5 misclassified. Thus by generating enough adversarial images, the adversary could expect to cause far more misclassification than would occur on natural inputs.
</p></blockquote>
<p>Other physical attacks mentioned in prior work include generation of audio inputs that mobile phones recognise as intelligible voice commands but humans hear as an unintelligible voice, and face recognition systems fooled by previously captured images of an authorized user&#8217;s face&#8230;</p>
<blockquote><p>
  An adversarial example for the face recognition domain might consist of very subtle markings applied to a person&#8217;s face, so that a human observer would recognize their identity correctly, but a machine learning system would recognize them as being a different person.
</p></blockquote>
<h3>Explaining and harnessing adversarial examples</h3>
<p>Why do these adversarial examples work? Goodfellow et al. show us that all we need in order to be vulnerable is <em>linear behavior in a high-dimensional space</em>.</p>
<blockquote><p>
  [The] results suggest that classifiers based on modern machine learning techniques, even those that obtain excellent performance on the test set, are not learning the true underlying concepts that determine the correct output label. Instead, these algorithms have built a Potemkin village that works well on naturally occuring data, but is exposed as a fake when one visits points in space that do not have high probability in the data distribution.
</p></blockquote>
<p>Consider a high-dimensional linear classifier, where the weight vector <em>w</em> has <em>n</em> dimensions. Each individual input feature has limited precision (e.g., using 8 bits per pixel in digital images, thus discarding all information below 1/255 of the dynamic range). For any one input, making a small change (smaller than the precision of the features) would not be expected to change the overall prediction of the classifier. However&#8230;</p>
<blockquote><p>
  &#8230; we can make many infinitesimal changes to the input that add up to one large change to the output. We can think of this as a sort of &#8216;accidental steganography,&#8217; where a linear model is forced to attend exclusively to the signal that aligns most closely with its weights, even if multiple signals are present and other signals have much greater amplitude.
</p></blockquote>
<p>We can maximise the impact of the many small changes by aligning the changes with the sign of the corresponding weight. This turns out to be a fast way of generating adversarial images.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-fig-1.jpeg?w=640" alt="" /></p>
<blockquote><p>
  An intriguing aspect of adversarial examples is that an example generated for one model is often misclassified by other models, even when they have different architecures or were trained on disjoint training sets. Moreover, when these different models misclassify an adversarial example, they often agree with each other on its class. Explanations based on extreme non-linearity and overfitting cannot readily account for this behavior&#8230;
</p></blockquote>
<p>But under the linear explanation, adversarial examples occur in broad subspaces &#8211; this explains why adversarial examples are abundant and why an example misclassified by one classifier has a fairly high probability of being misclassified by another. It&#8217;s the direction of perturbation, rather than the specific point in space, that matters most.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-fig-4.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-fig-4.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<blockquote><p>
  Our explanation suggests a fundamental tension between designing models that are easy to train due to their linearity and designing models that use nonlinear effects to resist adversarial perturbation. In the long run, it may be possible to escape this tradeoff by designing more powerful optimization methods that can successfully train more nonlinear models.
</p></blockquote>
<h3>Distillation as a defense to adversarial perturbations against deep neural networks</h3>
<p>Yesterday we looked at distillation as a way of transferring knowledge from large models to smaller models. In &#8216;Distillation as a defense&#8230;,&#8217; Papernot et al. show that the distillation technique (training using the probability distribution as the target, not just the argmax class label) can also be used to greatly reduce the vulnerability of networks to adversarial perturbations.</p>
<blockquote><p>
  We formulate a new variant of distillation to provide for defense training: instead of transferring knowledge between different architectures, we propose to use the knowledge extracted from a DNN to improve its own resilience to adversarial samples.
</p></blockquote>
<p>With a DNN trained on the MNIST dataset, defensive distillation reduces the success rate of adversarial sample crafting from 95.89% to just 0.45%! For a DNN trained on the CIFAR dataset, the success rate was reduced from 87.89% to 5.11%. In fact, defensive distillation can reduce the sensitivity of a DNN to input perturbations by a whopping factor of 10<sup>30</sup>. This increases the minimum number of input features that need to be perturbed for adversarial samples to succeed by up to 8x in tests.</p>
<p>Here are some examples from MNIST and CIFAR showing legitimate and adversarial samples:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-2.jpeg?w=480" alt="" /></p>
<p>So how and why does defensive distillation work? Consider a general adversarial crafting framework that works by first figuring out the <em>directions</em> around a given input sample in which the model learned by a DNN is most sensitive, and then uses this information to select a perturbation among the input dimensions.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-3.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-3.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<p>If the direction gradients are steep, we can make a big impact with small perturbations, but if they are shallow this is much harder to achieve. Think about the difference between being on a &#8216;ridge&#8217; in the classification space whereby a small move to either side could see you tumbling down the mountain, and being on a plateau where you can freely wander around without much consequence.</p>
<blockquote><p>
  To defend against such perturbations, one must therefore reduce these variations around the input, and consequently the amplitude of adversarial gradients. In other words, we must smooth the model learned during training by helping the network generalize better to samples outside of its training dataset.
</p></blockquote>
<p>The &#8216;robustness&#8217; of a DNN to adversarial samples is correlated with classifying inputs relatively consistently in the neighbourhood of a given sample.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-4.jpeg?w=480" alt="" /></p>
<p>To achieve this smoothing, distillation defense first trains a classification network as normal. Then we take another fresh model instance <em>with the exact same architecture</em> (no need to transfer to a smaller model) and train it using the probability vectors learned by the first model.</p>
<blockquote><p>
  The main difference between defensive distillation and the original distillation proposed by Hinton et al. is that we keep the same network architecture to train both the original network as well as the distilled network. This difference is justified by our end which is resilience instead of compression.
</p></blockquote>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-5.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-5.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<p>Training the network in this way with explicit relative information about classes prevents it from fitting too tightly to the data, and hence contributes to better generalization.</p>
<p>The following figure shows how the distillation <em>temperature</em> impacts the model&#8217;s ability to defend against adversarial samples. Intuitively, the higher the temperature the greater the smoothing, and thus the better the defence.</p>
<p><a href="https://adriancolyer.files.wordpress.com/2017/02/defensive-distillation-fig-7.jpeg"><img src="https://adriancolyer.files.wordpress.com/2017/02/defensive-distillation-fig-7.jpeg?w=640" alt="" /></a> (Click for larger view)</p>
<p>Distillation has only a small impact on classification accuracy, and <em>may even improve it!</em></p>
<p>We know that many different machine learning models are vulnerable to adversarial attacks, but the defensive distillation defense is only applicable to DNN models that produce an energy-based probability distribution for which a temperature can be defined&#8230;</p>
<blockquote><p>
  However, note that many machine learning models, unlike DNNs, don&#8217;t have the model capacity to be able to resist adversarial examples&#8230; A defense specialized to DNNs, guaranteed by the universal approximation property to at least be able to represent a function that correctly processes adversarial examples, is thus a significant step towards building machine learning models robust to adversarial samples.
</p></blockquote>
<p>This all sounds quite promising&#8230; unfortunately a subsequent paper showed that even defensive distillation is insufficient in mitigating adversarial examples :(, &#8216;<a href="https://arxiv.org/pdf/1607.04311.pdf">Defensive distillation is not robust to adversarial examples</a>.&#8217;</p>
<blockquote><p>
  In this short paper, we demonstrate that defensive distillation is not effective. We show that, with a slight modification to a standard attack, one can find adversarial examples on defensively distilled networks. We demonstrate the attack on the MNIST digit recognition task. Distillation prevents existing techniques from finding adversarial examples by increasing the magnitude of the inputs to the softmax layer. This makes an unmodified attack fail. We show that if we artificially reduce the magnitude of the input to the softmax function, and make two other minor changes, the attack succeeds. Our attack achieves successful targeted misclassification on 96.4% of images by changing on average 4.7% of pixels.
</p></blockquote>
<p>Damn!</p>
<h3>Vulnerability of deep reinforcement learning to policy induction attacks</h3>
<p>If you weren&#8217;t there already, this is where we get to the &#8216;Oh &#042;&#035;@!&#8217; moment! We&#8217;ve seen that classifiers can be fooled, but this paper and the next one show us that deep reinforcement learning networks (e.g. <a href="https://blog.acolyer.org/2016/03/02/graying-the-black-box-understanding-dqns/">DQNs</a>) are also vulnerable to adversarial attack. The attack is demonstrated on Atari games (what else!), but the broader implications are sobering:</p>
<blockquote><p>
  The reliance of RL on interactions with the environment gives rise to an inherent vulnerability which makes the process of learning susceptible to perturbation as a result of changes in the observable environment. Exploiting this vulnerability provides adversaries with the means to disrupt or change control policies, leading to unintended and potentially harmful actions. For instance, manipulation of the obstacle avoidance and navigation policies learned by autonomous Unmanned Aerial Vehicles (UAV) enables the adversary to use such systems as kinetic weapons by inducing actions that lead to intentional collisions.
</p></blockquote>
<p>Fortunately, we&#8217;ve already seen many of the building blocks needed to craft the attack, so we can describe it quite succinctly. The goal of the attacker is to fool a DQN into taking an action (inducing an arbitrary policy) chosen by the attacker. The assumed threat model is similar to the &#8216;black-box&#8217; model we saw earlier in this post: the attacker has no visibility of the insides of the DQN, and does not know its reward function. However, the attacker can see the same environmental inputs that the target DQN sees, and it can observe the actions taken by the DQN and hence estimate the reward function.</p>
<p>The first step is to us the &#8216;Practical black-box attack&#8230;&#8217; technique to train a substitute DQN that matches the policies chosen by the target. Following the black-box playbook we now craft adversarial <em>inputs</em> (instead of images) that trigger an incorrect choice of optimal action&#8230;</p>
<blockquote><p>
  If the attacker is capable of crafting adversarial inputs <em>s&#8217;<sub>t</sub></em> and <em>s&#8217;<sub>t+1</sub></em> such that the value of [the training function] is minimized for a specific action <em>a&#8217;</em>, then the policy learned by the DQN at this time-step is optimized for suggesting <em>a&#8217;</em> as the optimal action given the state <em>s<sub>t</sub></em>.
</p></blockquote>
<p>(An example of adversarial inputs might be manipulating some of the screen input pixels in an Atari game).</p>
<p>At this point we have a DQN which has learned an adversarial policy. The next step in the playbook is to find a way to <em>transfer</em> this learned adversarial policy to the target network. This is done in an <em>exploitation cycle:</em></p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vulnerability-of-drl-fig-2.jpeg?w=480" alt="" /></p>
<p>The first question we need to answer therefore, is &#8216;is it possible to generate adversarial examples for DQNs?&#8217; Fig. 4 below shows that yes, this is indeed possible (game of Atari Pong, using both the Fast Gradient Sign and Jacobian Saliency Map Algorithm approaches to generate adversarial perturbations)</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-4.jpeg?w=480" alt="" /></p>
<p>The next question we have to answer, is whether or not these adversarial examples can be transferred. The answer again is yes, with high success rate:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-5.jpeg?w=480" alt="" /></p>
<blockquote><p>
  Our final experiment tests the performance of our proposed exploitation mechanism. In this experiment, we consider an adversary whose reward value is the exact opposite of the game score, meaning that it aims to devise a policy that maximizes the number of lost games. To obtain this policy, we trained an adversarial DQN on the game, whose reward value was the negative of the value obtained from target DQN’s reward function&#8230;
</p></blockquote>
<p>A picture is worth a thousand words here:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-6.jpeg?w=480" alt="" /></p>
<p>Since all known counter-measures have been shown not to be sufficient,</p>
<blockquote><p>
  &#8230; it is hence concluded that the current state of the art in countering adverse examples and their exploitation is incapable of providing a concrete defense against such exploitations.
</p></blockquote>
<h3>Adversarial attacks on neural network policies</h3>
<p>Almost in parallel to the previous paper, Huang et al. published this work which also shows that reinforcement learning networks are vulnerable to adversarial attacks. They demonstrate this across <em>four</em> different Atari games (Chopper Command, Pong, Seaquest, and Space Invaders) using white-box attacks. They also show that the attacks succeed across a range of deep reinforcement learning algorithms (DQN, TRPO, and <a href="https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/">A3C</a>). Policies trained with TRPO and A3C are more resistant, but not safe from the attack.</p>
<p>Then the authors demonstrate transfer capabilities using black-box attacks too:</p>
<blockquote><p>
  We observe that the cross-dataset transferability property also holds in reinforcement learning applications, in the sense that an adversarial example designed to interfere with the operation of one policy interferes with the operation of another policy, so long as both policies have been trained to solve the same task. Specifically, we observe that adversarial examples transfer between models trained using different trajectory rollouts and between models trained with different training algorithms.
</p></blockquote>
<p>Combine this with the lessons we learned above in &#8216;Adversarial examples in the physical world,&#8217; and as the authors point out, things could get very interesting indeed!</p>
<blockquote><p>
  Our experiments show it is fairly easy to confuse such policies with computationally-efficient adversarial examples, even in black-box scenarios. Based on &#8216;Adversarial examples in the physical world&#8217;, it is possible that these adversarial perturbations could be applied to objects in the real world, for example adding strategically-placed paint to the surface of a road to confuse an autonomous car’s lane-following policy
</p></blockquote><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4055&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/feed/</wfw:commentRss>
            <slash:comments>1</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-armadillo.jpeg?w=200" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fours.jpeg?w=100" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-2.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-4.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-5.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-8.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/dnn-fooling-fig-14.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-panda.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-gibbon.jpeg?w=320" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/practical-black-box-fig-3.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/adversarial-physical-fig-3.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/explaining-adversarial-fig-4.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-2.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-3.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-4.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/distillation-defense-fig-5.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/defensive-distillation-fig-7.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vulnerability-of-drl-fig-2.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-4.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-5.jpeg?w=480" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vulnerability-drl-fig-6.jpeg?w=480" medium="image" />
        </item>
        <item>
            <title>Understanding, generalisation, and transfer learning in deep neural networks</title>
            <link>https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/</link>
            <comments>https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/#comments</comments>
            <pubDate>Mon, 27 Feb 2017 06:00:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4053</guid>
            <description><![CDATA[This is the first in a series of posts looking at the &#8216;top 100 awesome deep learning papers.&#8217; Deviating from the normal one-paper-per-day format, I&#8217;ll take the papers mostly in their groupings as found in the list (with some subdivision, plus a few extras thrown in) &#8211; thus we&#8217;ll be looking at multiple papers each [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4053&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>This is the first in a series of posts looking at the &#8216;<a href="https://github.com/terryum/awesome-deep-learning-papers">top 100 awesome deep learning papers</a>.&#8217; Deviating from the normal one-paper-per-day format, I&#8217;ll take the papers mostly in their groupings as found in the list (with some subdivision, plus a few extras thrown in) &#8211; thus we&#8217;ll be looking at multiple papers each day. The papers in today&#8217;s selection all shed light on what it is that DNNs (mostly CNNs) are really learning when trained. Since one way of understanding what a DNN has truly learned is to see how well the trained networks (or subsets of them) can perform on new tasks, we&#8217;ll also learn a lot about <em>generalization</em>, and what we learn can help us to define better models that take advantage of <em>transfer learning</em>.</p>
<p>The six papers we&#8217;ll look at today are:</p>
<ol>
<li><a href="http://arxiv.org/pdf/1311.2901">Visualizing and understanding convolutional networks</a>, Zeller &amp; Fergus 2013</li>
<li><a href="http://arxiv.org/pdf/1310.1531">DeCAF: A deep convolutional activation feature for generic visual recognition</a>, Donahue et al., 2014</li>
<li><a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf">CNN features off-the-shelf: an astounding baseline for recognition</a>, Razavian et al., 2014</li>
<li><a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">How transferable are features in deep neural networks?</a> Yosinski et al., 2014</li>
<li><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">Learning and transferring mid-level image representations using convolutional neural networks</a>, Oquab et al., 2014</li>
<li><a href="http://arxiv.org/pdf/1503.02531">Distilling the knowledge in a neural network</a>, Hinton et al., 2015</li>
</ol>
<p>I&#8217;ve done my best to distill the knowledge in these papers too, but inevitably this post is going to be a little longer than my normal target length! You might need one-and-a-half cups of coffee for this one ;).</p>
<h3>Visualising and understanding convolutional networks</h3>
<blockquote><p>
  Convolutional neural networks (convnets) have demonstrated excellent performance at tasks such as hand-written digit classification and face detection&#8230; Despite this encouraging progress, there is still little insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientific standpoint, this is deeply unsatisfactory.
</p></blockquote>
<p>If we&#8217;re going to understand what a convnet is doing, we need some way to map the feature activity in intermediate layers back into the input pixel space (we&#8217;re working with Convnets trained on the ImageNet dataset here). Zeiler and Fergus use a clever construction that they call a <em>deconvnet</em> that uses the same components as the convnet to be decoded, but in reverse order. Some of the convnet components need to be augmented slightly to capture additional information that helps in the reversing process. (It&#8217;s a little reminiscent of the <a href="https://blog.acolyer.org/2017/02/01/explaining-outputs-in-modern-data-analytics/">data flow provenance work</a> that we looked at earlier this year.)</p>
<p>Here&#8217;s an example with the standard convnet on the <em>right-hand side</em>, and the deconvnet layers added on the <em>left-hand</em> side.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vis-cnns-fig-1.jpeg?w=600" alt="" /></p>
<p>Now that we can project from any layer back onto pixels, we can get a peek into what they seem to be learning. This leads to now-familiar pictures such as this:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/vis-cnns-fig-2.jpeg?w=600" alt="" /></p>
<p>Note in the above how layer 2 responds to corners and edge/colour combinations, layer 3 seems to capture similar textures, layer 4 is more class-specific (e.g. dog faces), and layer 5 shows entire objects with significant pose variation. Using these visualisations and looking at how they change over time during training it is also possible to see lower layers converging within relatively few epochs, whereas upper layers take considerably longer to converge. Small transformations in the input image have a big effect on lower layers, but lesser impact in higher layers.</p>
<p>The understanding gleaned from inspecting these visualisations proved to be a helpful tool for improving the underlying models too. For example, a 2nd layer visualization showed aliasing artefacts caused by a large stride size, reducing the stride size gave an increase in classification performance.</p>
<p>Experiments with model structure showed that having a minimum depth to the network, rather than any one specific section in the overall model, is vital to model performance.</p>
<h3>DeCAF: A deep convolutional activation feature for generic visual recognition</h3>
<p>Many visual recognition challenges have data sets with comparatively few examples. In DeCAF, the authors explore whether a convolutional network trained on ImageNet (a large dataset) can be generalised to other tasks where less data is available:</p>
<blockquote><p>
  Our model can either be considered as a deep architecture for transfer learning based on a supervised pre-training phase, or simply as a new visual feature <em>DeCAF</em> defined by the convolutional network weights learned on a set of pre-defined object recognition tasks.
</p></blockquote>
<p>After training deep a convolutional model (using Krizhevskey et al.&#8217;s competition winning 2012 architecture), features are extracted from the resulting model and used as inputs to generic vision tasks. Success in those task would indicate that the convolutional network is learning generically useful features of images (in much the same way that <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">word embeddings learn features of words</a>).</p>
<p>Let DeCAF<sub>n</sub> be the activations of the nth hidden layer of the CNN. DeCAF<sub>7</sub> is the final hidden layer just before propagating through the last fully connected layer to produce class predictions. All of the weights from the CNN up to the layer under test are frozen, and either a logistic regression or support vector machine is trained using the CNN features as input.</p>
<p>On the <a href="https://en.m.wikipedia.org/wiki/Caltech_101">Caltech-101 dataset</a> the DeCAF<sub>6</sub> + SVM outperformed the previous best state of the art (a method with a combination of five traditional hand-engineered image features)!</p>
<p>The <em>Office</em> dataset contains product images from amazon.com, as well as images taken in office environments using webcams and DSLRs. DeCAF features were shown to be robust to resolution changes (webcam vs DSLR), providing not only better <em>within</em> category clustering, but also was able to cluster same category instances across domains. DeCAF + SVM again dramatically outperformed the baseline SURF features available with the Office dataset.</p>
<p>For sub-category recognition (e.g. distinguishing between lots of different bird types in the Caltech-UCSD birds dataset) DeCAF<sub>6</sub> with simple logistic regression again obtained a significant increase over existing approaches: &#8220;To the best of our knowledge, this is the best accuracy reported so far in the literature.&#8221;</p>
<p>And finally, for scene recognition tasks, DeCAF + logistic regression on the SUN-397 large-scale scene recognition database also outperformed the current state-of-the-art.</p>
<p>Convolution neural networks trained on large image sets were therefore forcefully demonstrated to learn features with sufficient representational power and generalization ability to perform at state-of-the-art levels on a wide variety of image-based tasks. It&#8217;s the beginning of the end of hand-engineered features, and welcome to the era of deep-learned features.</p>
<blockquote><p>
  The ability of a visual recognition system to achieve high classification accuracy on tasks with sparse labeled data has proven to be an elusive goal in computer vision research, but our multi-task deep learning framework and fast open-source implementation are significant steps in this direction.
</p></blockquote>
<h3>CNN features off-the-shelf: an astounding baseline for recognition</h3>
<p>CNN features off the shelf further reinforces that we can learn general features useful for image-based tasks, and apply them very successfully in new domains. This time the baseline features are taken from a trained convolutional neural network model called <em>Overfeat</em>, which has been optimized for object image classification in ILSVRC. Then for a variety of tasks, instead of using state-of-art image processing pipelines, the authors simply take the features from the CNN representation, and bolt on an SVM. Sounds familiar?</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/cnn-ots-fig-1.jpeg?w=600" alt="" /></p>
<p>The tasks undertaken progress from quite close to the original classification task, to more and more demanding (i.e. distant tasks). At every step of the way, the CNN features prove their worth!</p>
<h4>Step 1: Object and scene recognition</h4>
<p>The Pascal VOC 2007 dataset has ~10,000 images of 20 classes of animals, and is considered more challenging than ILSVRC. Applying the Overfeat CNN features to this dataset resulted in a model outperforming all previous efforts &#8220;by a significant margin.&#8221; The following chart shows how classification performance improves depending on the level from the original CNN that is chosen as the input to the final SVM:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/cnn-ots-fig-2a.jpeg?w=360" alt="" /></p>
<p>For scene recognition, the MIT-67 indoor scene dataset has 15,620 images of 67 indoor scene classes. The CNN + SVM model significantly outperformed a majority of the baseline models and just edges a state-of-the-art award by 0.1% accuracy over the previous best AlexConvNet model (also a CNN).</p>
<h4>Step 2: Fine-grained recognition</h4>
<p>Here we&#8217;re back with birds (Caltech UCSD 200-2011 dataset) and also flowers (Oxford 102 flowers dataset). Can the more generic OverFeat features pick up potentially subtle differences between the very similar classes? On the birds dataset the model gets very close to the state of the art (also a CNN), and beats all other baselines. On the flowers dataset, the CNN+SVM model outperforms the previous state-of-the-art.</p>
<h4>Step 3: Attribute detection</h4>
<p>Have the OverFeat features encoded something about the semantic properties of people and objects? The H3D dataset defines 9 attributes for person images (for example, &#8216;has glasses,&#8217; and &#8216;is male.&#8217;). The UIUC 64 dataset has attributes for objects (e.g., &#8216;is 2D boxy&#8217;, &#8216;has head&#8217;, &#8216;is furry&#8217;). The CNN-SVM achieves state of the art on UIUC 64, and beat several existing models on H3D.</p>
<h4>Step 4: Instance retrieval</h4>
<p>What about trying the CNN-SVM model on instance retrieval problems? This is a domain where the state-of-the-art using highly optimized engineered vectors and mid-level features. Against methods that do <em>not</em> incorporate 3D geometric constraints (which do better), the CNN features proved very competitive on building and holiday datasets.</p>
<h4>What have we learned?</h4>
<blockquote><p>
  It&#8217;s all about the features! SIFT and HOG descriptors produced big performance gains a decade ago and now deep convolutional features are providing a similar breakthrough for recognition. Thus, applying the well-established computer vision procedures on CNN representations should potentially push the reported results even further. In any case, if you develop any new algorithm for a recognition task, it <strong>must</strong> be compared against the strong baseline of <em>generic deep features + simple classifier.</em>
</p></blockquote>
<h3>How transferable are features in deep neural networks?</h3>
<p>The previous papers mostly focused on taking the higher layers from the pre-trained CNNs as input features. In &#8216;How transferable are features in deep neural networks&#8217; the authors systematically explore the generality of the features learned at each layer &#8211; and as we&#8217;ve seen, to the extent that features at a given layer <em>are</em> general, we&#8217;ll be able to use them for transfer learning.</p>
<blockquote><p>
  The usual transfer learning approach is to train a base network and then copy its first <em>n</em> layers to the the first <em>n</em> layers of a target network. The remaining layers of the target network are then randomly initialized and trained toward the target task. One can choose to back-propagate the errors from the new task into the base (copied) features to <em>fine-tune</em> them to the new task, or the transferred feature layers can be left <em>frozen</em>&#8230;
</p></blockquote>
<p>The experiment setup is really neat. Take an 8-layer CNN model, and split the 1000 ImageNet <em>classes</em> into two groups (so that each contains approximately half the data or 645,000 examples). Train one instance of the model on half <strong>A</strong>, and call it <strong>baseA</strong>. Train another instance of the model on half <strong>B</strong>, and call it <strong>baseB</strong>. Starting with baseA, we can define seven starter networks, A1 through A7, that copy the first 1 through 7 layers from baseA respectively (and of course we can do the same from baseB to give B1 through B7). Say we&#8217;re interested in exploring how well features learned at layer 3 transfer. We can construct the following four networks:</p>
<ul>
<li>B3B &#8211; the first 3 layers are copied from baseB and frozen. The remaining five higher layers are initialized randomly and we train on task B as a control. (The authors call this a &#8216;selfer&#8217; network)</li>
<li>A3B &#8211; the first 3 layers are copied from baseA and frozen. The remaining five layers are initialized randomly as before, and trained on task B. If A3B performs as well as B3B, we have evidence that the first three layers are general.</li>
<li>B3B+, like B3B but the first three layers are subsequently fine-tuned during training.</li>
<li>A3B+, like A3B but the first three layers are subsequently fine-tuned during training.</li>
</ul>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/cnn-transferable-fig-1.jpeg?w=600" alt="" /></p>
<p>Repeat this process for all layers 1..7. Running these experiments leads to the following results:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/cnn-transferable-fig-2.jpeg?w=600" alt="" /></p>
<p>Looking at the dark blue dots first (BnB), we see an interesting phenomenon. When freezing early layers and then retraining the later layers towards the same task, the resulting performance is very close to baseB. But layers 3,4,5, and 6 (especially 4 and 5) show significantly worse performance:</p>
<blockquote><p>
  This performance drop is evidence that the original network contained <em>fragile co-adapted features</em> on successive layers.
</p></blockquote>
<p>As we get closer to the final layers, performance is restored as it seems there is less to learn&#8230; &#8220;<em>To our knowledge it has not been previously observed in the literature that such optimization difficulties may be worse in the middle of a network than near the bottom or top</em>.&#8221;</p>
<p>Note that the light blue dots (BnB+), where we allow fine-tuning, restore full performance.</p>
<p>The red dots show the transfer learning results. Starting with the frozen base version (dark red, AnB), we see strong transference in layers 1 and 2, and only a slight drop in layer 3, indicating that the learned features are general. Through layers 4-7 though we see a significant drop in performance.</p>
<blockquote><p>
  Thanks to the BnB points, we can tell that this drop is from a combination of two separate effects: the drop from lost co-adaptation <em>and</em> the drop from features that are less and less general.
</p></blockquote>
<p>Finally let&#8217;s look at the light red AnB+ points. These do <em>better</em> than the baseline! Surprised? Even when the dataset is large, transferring features seems to boost generalization performance. Keeping anywhere from one to seven layers seems to infer some benefit (average boost 1.6%) so the effect is seen everywhere. One way that I think about this is that the transferred layers had a chance to learn from different images that the selfer networks never see &#8211; thus they have a better chance of learning better generalizations.</p>
<p>The short summary &#8211; transfers can improve generalization performance. Two issues impact how well transfer occurs: fragile co-adaptation of middle layers, and specialisation of higher layers.</p>
<h3>Learning and transferring mid-level image representations using convolutional neural networks</h3>
<p>This paper uses the by-now familiar &#8216;train a CNN on ImageNet and extract features to transfer to other tasks approach, but also explores training techniques that can help to maximise the transfer benefits. Here&#8217;s the setup:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-2.jpeg?w=600" alt="" /></p>
<p>The target task for transfer learning is Pascal VOC object and action classification, &#8220;we wish to design a network that will output scores for target categories, or <code>background</code> if none of the categories are present in the image.&#8221; Transfer is achieved by removing the last fully-connected layer from the pre-trained network and adding an <em>adaption layer</em> formed of two fully-connected layers.</p>
<p>The source dataset (ImageNet) contains nice images of single centered objects. The target dataset (Pascal VOC) contains complex scenes with multiple target objects at various scales and background clutter:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-3.jpeg?w=600" alt="" /></p>
<blockquote><p>
  The distributed of object orientations and sizes as well as, for example, their mutual occlusion patterns is very different between the two tasks. This issue has also been called &#8220;a dataset capture bias.&#8221; In addition, the target task may contain many other objects not present in the source task training data (&#8220;a negative bias&#8221;).
</p></blockquote>
<p>Here&#8217;s the new twist: to address these biases, the authors use a sliding window and extract around 500 square patches from each image by sampling on eight different scales using a regularly spaced grid and 50% or more overlap between neighbouring patches:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-4.jpeg?w=600" alt="" /></p>
<p>To label the patches in the resulting training data, the authors measure the overlap between the bounding box of a patch P and the ground truth bounding boxes of annotated objects in the image. If there is sufficient overlap with a given class, the patch is labelled as a positive training example for the class.</p>
<p>You shouldn&#8217;t be surprised at this point to learn that the resulting network achieves state of the art performance on Pascal VOC 2007 object recognition, and gets very close to the state of the art on Pascal VOC 2012. The authors also demonstrate that the network learns about the size and location of target objects within the image. For the Pascal VOC 2012 <em>action</em> recognition task, state of the art results were achieved by allowing fine-tuning of the copied layers during training.</p>
<blockquote><p>
  Our work is part of the recent evidence that convolutional neural networks provide means to learn rich mid-level image features transferrable to a variety of visual recognition tasks.
</p></blockquote>
<h3>Distilling the knowledge in a neural network</h3>
<p>Let&#8217;s finish with something a little bit different: what can insects teach us about neural network training and design?</p>
<blockquote><p>
  Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction. In large-scale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements.
</p></blockquote>
<p>What if we use large cumbersome models during training (e.g.,very deep networks, ensembles), so long as those models make it easier to extract structure from the training data, and then find a way to transfer or <em>distill</em> that the training model has learned into a a more compact form suitable for deployment? We want to cram as much of the knowledge as possible from the large model into the smaller one.</p>
<blockquote><p>
  If the cumbersome model generalizes well because, for example, it is the average of a large ensemble of different models, a small model trained to generalize in the same way will typically do much better on test data than a small model that is trained in the normal way on the same training set as was used to train the ensemble.
</p></blockquote>
<p>How can we train the small model effectively though? <em>By using the class probabilities produced by the cumbersome model as &#8220;soft targets&#8221; for training the small model.</em> The large model has learned not just the target prediction class, but a probability distribution over all classes &#8211; and the relative probabilities of incorrect answers still contain a lot of valuable information. The essence of the idea is to train the small model to reproduce the <em>probability distribution</em>, not just the target output class.</p>
<blockquote><p>Neural networks typically produce class probabilities by using a &#8220;softmax&#8221; output layer that converts the logit, _z<sub>i</sub>_, computed for each class into a probability, _q<sub>i</sub>_, by comparing _z<sub>i</sub>_ with the other logits.</p></blockquote>
<p><img src="https://s0.wp.com/latex.php?latex=q_i+%3D+%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_j%7B%5Cexp%28z_j%2FT%29%7D%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0" alt="q_i = &#92;frac{&#92;exp(z_i/T)}{&#92;sum_j{&#92;exp(z_j/T)}}" title="q_i = &#92;frac{&#92;exp(z_i/T)}{&#92;sum_j{&#92;exp(z_j/T)}}" class="latex" /></p>
<p>where <em>T</em> is a temperature normally set to 1. A higher T value produces a softer probability distribution over classes. The cumbersome model is trained using a high temperature in its softmax, and the same high temperature is used when training the distilled model. When that model is <em>deployed</em> though, it uses a temperature of 1.</p>
<p>A &#8216;cumbersome&#8217; large neural net with two hidden layers of 1200 rectified linear units trained on 60,000 training cases using dropouts to simulate training an ensemble of models sharing weights,achieved only 67 test errors. A smaller model network with 800 units in each layer and no regulalization saw 146 test errors. However, a <em>distilled</em> smaller network of the same size trained to match the soft targets from the large network achieved only 74 test errors.</p>
<p>In an Automatic Speech Recognition (ASR) test an ensemble of 10 models was distilled to a single model that performed almost as well. The results compare well to a very strong baseline model similar to that used by Android voice search.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/distilling-knowledge-table-1.jpeg?w=600" alt="" /></p>
<p>A very nice use of the technique is in <em>learning specialized models</em> as part of an ensemble. Take a large data set (e.g. Google&#8217;s JFT image dataset with 100M images) and a large number of labels (15,000 in JFT): it&#8217;s likely there are several subsets of labels on which a general model gets confused. Using a clustering algorithm to find classes that are often predicted together, an ensemble is created with one generalist model, and many specialist models, one for each of the top k clusters. The specialist models are trained on data highly enriched in examples from the confusable subsets. The hope is that the resulting knowledge can be distilled back into a single large net, although the authors did not demonstrate that final step in the paper.</p>
<blockquote><p>
  We have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model&#8230;. [Furthermore,] we have shown that the performance of a single really big net that has been trained for a long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster.
</p></blockquote>
<h3>Understanding through counter-examples</h3>
<p>Another interesting way of understanding what DNNs have learned, is through the discovery of counter-examples that confuse them. The &#8216;<a href="https://github.com/terryum/awesome-deep-learning-papers">top 100 awesome deep learning papers</a>&#8216; section on understanding, generalisation, and transfer learning (which we&#8217;ve been working through today) contains one paper along those lines. But this post is long enough already, and the subject is sufficiently interesting that I&#8217;d like to expand it with a few additional papers as well. So we&#8217;ll look at that tomorrow&#8230;</p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4053&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/feed/</wfw:commentRss>
            <slash:comments>1</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vis-cnns-fig-1.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/vis-cnns-fig-2.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/cnn-ots-fig-1.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/cnn-ots-fig-2a.jpeg?w=360" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/cnn-transferable-fig-1.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/cnn-transferable-fig-2.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-2.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-3.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/mid-level-cnn-transfer-fig-4.jpeg" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/distilling-knowledge-table-1.jpeg" medium="image" />
        </item>
        <item>
            <title>An experiment with awesome deep learning papers</title>
            <link>https://blog.acolyer.org/2017/02/26/an-experiment-with-awesome-deep-learning-papers/</link>
            <comments>https://blog.acolyer.org/2017/02/26/an-experiment-with-awesome-deep-learning-papers/#comments</comments>
            <pubDate>Sun, 26 Feb 2017 16:30:00 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Machine Learning]]></category>

            <guid isPermaLink="false">http://adriancolyer.wordpress.com/?p=4063</guid>
            <description><![CDATA[There have been several lists of deep learning papers doing the rounds. Recently Terry Taewoong Um&#8217;s list of the top 100 awesome and most cited deep learning papers caught my eye. Deep learning is an exciting area and it&#8217;s moving fast. I&#8217;d like to know what&#8217;s in those 100 papers (thankfully, we have at least [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4063&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p>There have been several lists of deep learning papers doing the rounds. Recently Terry Taewoong Um&#8217;s list of the <a href="https://github.com/terryum/awesome-deep-learning-papers">top 100 awesome and most cited deep learning papers</a> caught my eye. Deep learning is an exciting area and it&#8217;s moving fast. I&#8217;d like to know what&#8217;s in those 100 papers (thankfully, we have at least looked at <em>some</em> of them before), and I suspect many of you would too. The problem is, at five papers per week it would take 20 weeks of The Morning Paper to cover the list! That&#8217;s too long, and even if it wasn&#8217;t, it wouldn&#8217;t match my goals for diversity of content. Ideally I&#8217;d press pause on the world for a while, <a href="http://datascience.ibm.com/blog/the-mathematics-of-machine-learning/">go take a bunch of math classes</a>, read through all of these papers, and then be in a strong position to understand the 2017 intake. That&#8217;s not possible either, so we need a faster plan&#8230;</p>
<p><em>This week on The Morning Paper therefore I&#8217;m trying something a little different: we&#8217;ll be working through some of the top 100 papers list in sections, covering multiple papers per day but with shorter reviews of each. Hopefully you&#8217;ll find it a good way to take on board a lot of research very quickly.</em></p>
<p>From my perspective the results are mixed &#8211; I certainly covered a lot of ground (26 deep learning papers read and reviewed over the week), but I think I discovered my current limit for paper reviews per week in the process! (It happened to coincide with a a busy work week as well). Another consequence is that the posts are longer than my usual target length, coming in at about 15 mins reading time according to my editor&#8217;s toolbar. I fear that may be too long, even in condensed review format. Let me know what you think.</p>
<p>One thing&#8217;s for sure, for the following week I&#8217;m going back to just one paper per day thank you very much! (And we&#8217;ll be looking at something other than deep learning too <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" alt="😉" class="wp-smiley" style="height: 1em; max-height: 1em;" /> ).</p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=4063&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/02/26/an-experiment-with-awesome-deep-learning-papers/feed/</wfw:commentRss>
            <slash:comments>5</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>
        </item>
        <item>
            <title>On decentralizing prediction markets and order books</title>
            <link>https://blog.acolyer.org/2017/02/24/on-decentralizing-prediction-markets-and-order-books/</link>
            <comments>https://blog.acolyer.org/2017/02/24/on-decentralizing-prediction-markets-and-order-books/#comments</comments>
            <pubDate>Fri, 24 Feb 2017 06:00:52 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Uncategorized]]></category>

            <guid isPermaLink="false">http://blog.acolyer.org/?p=3977</guid>
            <description><![CDATA[On decentralizing prediction markets and order books Clark et al., 13th Annual Workshop on the Economics of Information Security, 2014 This is the last of five papers in the ACM Queue Research for Practice series on &#8216;Cryptocurrencies, Blockchains, and Smart Contracts .&#8217; It serves as a good example of repurposing block chains as a foundation [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=3977&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p><a href="http://www.econinfosec.org/archive/weis2014/papers/Clark-WEIS2014.pdf">On decentralizing prediction markets and order books</a>  Clark et al., <em>13th Annual Workshop on the Economics of Information Security, 2014</em></p>
<p>This is the last of five papers in the ACM Queue <em>Research for Practice</em> series on &#8216;<a href="http://queue.acm.org/detail.cfm?id=3043967">Cryptocurrencies, Blockchains, and Smart Contracts</a> .&#8217;  It serves as a good example of repurposing block chains as a foundation not just for cryptocurrencies, but also as a means of providing decentralized versions of a wide range of services. In this instance, a prediction market.</p>
<blockquote><p>
  Among [Bitcoin&#8217;s] novel contributions, the block chain stands out as a useful component for forming a consensus within a decentralized network about efficiently decideable events. Repurposing this consensus mechanism for new uses, both related and non-related to finance, is a promising research direction. In our present work, we show it can be repurposed to deploy a prediction market &#8211; a useful tool for forecasting future events.</p></blockquote>
<p>All of these systems have a really interesting game theory angle to them &#8211; you need to define a system in which the best strategy for each party (i.e, when acting in their own best interests), is also the best overall strategy for the system as a whole. It&#8217;s not easy to get that right!</p>
<h3>What is a prediction market?</h3>
<p>A <em>prediction market</em> enables participants to trade financial shares tied to the outcome of a future event. The possible outcomes must be distinct and must partition the outcome space. Suppose Clump and Trinton are both running for president. A share in &#8216;Clump becomes president&#8217; is worth $1 if Clump wins, and $0 otherwise. If participants believe Clump has a 60% chance of winning, the share has an expected value of $0.60.</p>
<blockquote><p>
  All deployed prediction markets that we are aware of are run by a trusted central authority, who holds the money and the shares backing the market in escrow while providing an electronic communication network capable of matching orders and posting the bid/ask of unmatched orders. Orders that are executed are cleared and settled by the authority, and when the events being forecast are realized, the authority declares the outcome and settles the accounts.</p></blockquote>
<p>There are two key components to a market: an <em>order matching service</em>  matches buyers and sellers that are willing to trade, and a <em>processor</em> enables the execution of valid trades (regardless of how they are arranged) in a timely fashion.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/prediction-markets-fig-1.jpeg?w=640" alt=""></p>
<h3>Design goals for a <em>decentralized</em> prediction market</h3>
<p>In the quote above, notice the central role of &#8216;the authority.&#8217; We want to design a prediction market that doesn&#8217;t have any central authority, or at least not for the core concerns&#8230;</p>
<blockquote><p>
  Our design <em>decentralizes</em> order matching, settling, and clearing to a community of miners, where authority is held in proportion to computational ability. We argue that trusted authorities, called arbiters, are better suited to declare the outcome of an event than through community consensus, however users can adaptively move between markets to ensure they only rely on arbiters they trust (<em>trust agility</em>). Arbitration is also desgined to have a <em>minimal barrier to entry</em>&#8230;</p></blockquote>
<p>All transactions are published in a public block chain which provides transparency, and accounts are identified by public keys providing pseudonymity.</p>
<ul>
<li><strong>Traders</strong> buy and sell shares in outcomes. They may be malicious and attempt to steal shares, double-spend currency to obtain shares, and redeem shares in outcomes other than the declared outcome.</li>
<li><strong>Miners</strong> maintain a transcript of all transactions as they do in Bitcoin</li>
<li><strong>Arbiters</strong> are entities authorized to declare the outcome of an event&#8230;</li>
</ul>
<blockquote><p>
  Arbiters pose the most significant threat. A malicious arbiter can profit directly from misbehaviour, for example, by obtaining cheap shares in a low probability outcome and falsely declaring that outcome the winner. Arbiters may also be simply unreliable, and not declare outcomes in a timely fashion or at all. While these threats cannot be eliminated, we use trust, agility, reputation, and community override to control and mitigate them.</p></blockquote>
<h3>Clearing and settlement</h3>
<p>Decentralized clearing and settlement takes place by extending an altcoin (with internal currency XFT) with five additional operations: OpenMarket, BuyCompleteSets, SellCompleteSets, Exchange, and CloseMarket. Here&#8217;s the summary description of the operations from the paper:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/prediction-markets-construction-1.jpeg?w=640" alt=""></p>
<p>OpenMarket and CloseMarket are fairly self-explanatory, but the notion of &#8216;complete sets&#8217; is worth exploring. BuyCompleteSets enables a trader to buy a share in each outcome for 1 XFT, and SellCompleteSets enables a trader holding a share in each outcome to sell the set for 1 XFT. This mechanism ensures that over the long run, 1 XFT remains within the bid-ask spread of a complete set. For example, if the bid prices add up to 1.05 XFT, a trader can buy a complete set of 1 XFT and sell the individual shares for 1.05 XFT, netting a 0.05 XFT profit. Likewise if the ask prices add up to 0.98 XFT, a trader can buy individual shares to accumulate a complete set for 0.98 XFT, and then sell the complete set for 1 XFT netting 0.02 XFT profit.</p>
<p>Here&#8217;s an example of &#8216;going short&#8217; that illustrates some of these mechanisms at work:</p>
<blockquote><p>
  Alice believes candidate Thomas Fredrick is over-valued at {Bid 0.24, Ask: 0.27, Last: 0.26} for becoming his political party’s presidential nominee. She purchases a portfolio using BuyCompleteSets for 1 XFT and immediate sells the share in Fredrick for 0.24 XFT. Two weeks later, Fredrick drops out of the race, and his share price plummets to an Ask of 0.001. As long as Fredrick stays out of the race, Alice owns a share in each other option and can expect 1 XFT when the primary finishes in two months. However she will earn nothing (not even interest) between now and then. Alice purchases a share in Fredrick for 0.001 to complete her portfolio and uses SellCompleteSets to receive 1 XFT immediately. Assuming no transaction fees, Alice initial investment of 1 XFT earned her 1 + 0.24 − 0.001 = 1.239 XFT.</p></blockquote>
<p>The central and most complex transaction is the Exchange transaction. Like a Bitcoin transaction, it has a unique identifier, a set of inputs, and a set of outputs. Inputs refer outputs of previous transactions as in Bitcoin. Inputs and outputs can be one of four different types:</p>
<ol>
<li>XFT (i.e. Basecoin)</li>
<li>Shares in an active market</li>
<li>Shares from a closed market</li>
<li>Portfolios (complete sets)</li>
</ol>
<p>Inputs must completely spend the output they refer to, and the sum of the inputs must meet or exceed the sum of the outputs. Any   input excess above the sum of the inputs is kept by the miner as a transaction fee. Outputs are assigned to public keys, and an Exchange transaction must be signed with the signing keys associated with each of its inputs.</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/prediction-markets-fig-2.jpeg?w=640" alt=""></p>
<blockquote><p>
  &#8230;it is clear that for most markets arbitration requires one or more humans in the loop and cannot be automated, nor can the correctness of arbitration be defined or checked automatically. Nevertheless, the problem at least theoretically has a solution if the majority of participants are honest. This is complicated by three factors: first, participants may be pseudonymous, and anyone can create sybils. Second, some participants will have a monetary stake in subverting the vote because they hold shares in a losing outcome. Third, we would like to minimize the human effort required to reach consensus.</p></blockquote>
<p>The authors explore four different models for arbitration, the simplest being an arbiter per market. The alternatives explored including (i) giving every miner who broadcasts a block a vote in the market (works best in prominent markets where a significant fraction of miners are likely to record votes), (ii) creating virtual companies (with shares also traded on the block chain) to adjudicate markets &#8211; the value of their shares is tied to the expected future earnings, which is tied to reputation, encouraging the shareholders to vote honestly, and (iii) a model in which each of <em>N</em> shares in a prediction market confers one vote&#8230;</p>
<blockquote><p>
  Of course, voters holding losing shares have a financial incentive to vote contrary to reality. To address this dilemma, all market participants are required to post a bond in addition to the price of the shares they purchase. Any voters who vote contrary to an outcome with reaches a 2N/3 consensus forfeit their bond, disincentivizing voters to vote against the likely final outcome.</p></blockquote>
<h3>Order matching</h3>
<p>The most common type of order matching system in prediction markets is a continuous two-sided auction: traders submit bid or ask orders, and an order is executed if some other trader is willing to match (or better) its conditions. Orders are executed in a sequence specified by precedence rules &#8211; for example, first sorted by best price, then by time received. With a decentralized blockchain though, any time-based precedence is hard to establish and open to manipulation (remember <a href="https://blog.acolyer.org/2017/02/23/making-smart-contracts-smarter">miners have considerable flexibility with timestamps</a>). Traders also have an incentive to configure any nodes they own in the network not to forward orders at higher trade precedence than their own.</p>
<p>The solution is to give precedence to <em>price</em>, and then execute orders at the same price relative to their <em>volume</em>.  The mitigation for traders not forwarding orders is simply to broadcast the message to as many peers as possible. Because transactions aren&#8217;t finalized until they&#8217;re added to the block chain we also need to use a <em>call</em> market instead of a <em>continuous</em> market. Orders are placed over an interval of time, and then executed as a batch.</p>
<blockquote><p>
  The only limitation to a matching service is that clearing and settlement requires block chain integration, and the timing of integration depends on the average block creation time. With a 6 block confirmation delay at 10 minutes a block (as per Bitcoin), we are left with a reasonable period of one hour. By comparison, equities often take one or three days (‘T+1’ or ‘T+3’) to clear and settle. However an external exchange that holds the shares and XFT for its traders can clear and settle immediately, at least in terms of the traders’ accounts with the exchange (while actually withdrawing the shares or XFT from the exchange is subject to the same block chain delay).</p></blockquote>
<h3>Wrapping up</h3>
<p>I skipped over a lot of detailed analysis of the incentives for the various parties (traders, miners, and arbiters), ways they can try to game the system, and how these can be protected against (the full paper runs to 21 pages). The takeaway is that it takes a lot of careful analysis to design such systems. It leaves me wanting something more rigorous than prose for reasoning about them!</p>
<p>Let&#8217;s conclude by looking at the other side of the coin: a prediction market with no central authority gains the benefits of not having a central authority, as well as the potential drawbacks:</p>
<blockquote><p>
  In centralized PMs, the decision to allow any market is discretionary. In decentralized systems, there is little to no control over what types of markets will be opened. Markets on assassinations or terrorist attacks can be considered unethical, and were cited as reasons for cancelling at least one prediction market, developed in the US by DARPA for predicting foreign political developments. In our design, for particularly abhorrent markets, a consensus formed by a majority of miners to not include transactions opening, trading, or closing a given market will effectively stifle it. However markets with merely questionable ethics will likely proceed, and it will be left to individual users to decide to participate or not.</p></blockquote><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=3977&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/02/24/on-decentralizing-prediction-markets-and-order-books/feed/</wfw:commentRss>
            <slash:comments>1</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/prediction-markets-fig-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/prediction-markets-construction-1.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/prediction-markets-fig-2.jpeg?w=640" medium="image" />
        </item>
        <item>
            <title>Making smart contracts smarter</title>
            <link>https://blog.acolyer.org/2017/02/23/making-smart-contracts-smarter/</link>
            <comments>https://blog.acolyer.org/2017/02/23/making-smart-contracts-smarter/#comments</comments>
            <pubDate>Thu, 23 Feb 2017 06:00:33 +0000</pubDate>
            <dc:creator><![CDATA[adriancolyer]]></dc:creator>
            <category><![CDATA[Uncategorized]]></category>

            <guid isPermaLink="false">http://blog.acolyer.org/?p=3968</guid>
            <description><![CDATA[Making smart contracts smarter Luu et al., CCS 2016 This is the fourth in a series of papers from the ACM Queue Research for Practice &#8216;Cryptocurrencies, Blockchains and Smart Contracts&#8216; selections, in which Luu at al. look at smart contracts in Ethereum. Smart contracts are a really intriguing idea and have generated a lot of [&#8230;]<img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=3968&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></description>
            <content:encoded><![CDATA[<p><a href="https://dl.acm.org/citation.cfm?id=2978309">Making smart contracts smarter</a> Luu et al., <em>CCS 2016</em></p>
<p>This is the fourth in a series of papers from the ACM Queue Research for Practice &#8216;<a href="http://queue.acm.org/detail.cfm?id=3043967">Cryptocurrencies, Blockchains and Smart Contracts</a>&#8216; selections, in which Luu at al. look at smart contracts in Ethereum. Smart contracts are a really intriguing idea and have generated a lot of interest/excitement, but they also have a number of properties which make them both likely targets for attackers and also hard to get right. Regular readers of The Morning Paper will not be surprised to see our old friend <a href="https://blog.acolyer.org/2016/10/06/simple-testing-can-prevent-most-critical-failures/">error and exception handling popping up as one of the chief causes of problems</a> again! After scanning 19,366 Ethereum contracts using the OYENTE tool described in the paper, the authors found vulnerabilities in 8,833 of them.</p>
<p>Here&#8217;s the plan: after a brief introduction to smart contracts, we&#8217;ll discuss what it is that makes them especially attractive targets, followed by a look at typical vulnerabilities. We&#8217;ll then finish up by seeing what we can do about the situation to make contracts more secure in the future.</p>
<h3>What exactly <em>is</em> a smart contract?</h3>
<blockquote><p>
  A smart contract is identified by an address (a 160-bit identifier) and its code resides on the blockchain. Users invoke a smart contract in present cryptocurrencies by sending transactions to the contract address. Specifically, if a new transaction is accepted by the blockchain and has a contract address as the recipient, then all participants on the mining network execute the contract code with the current state of the blockchain and the transaction payloads as inputs. The network then agrees on the output and the next state of the contract by participating in a consensus protocol.
</p></blockquote>
<p>In Ethereum, contracts are introduce to the blockchain via special <em>creation</em> transactions. Contracts are essentially functions whose Ethereum Virtual Machine (EVM) bytecode is incorporated in the blockchain as part of the creation transaction. The contracts themselves can be written in higher-level languages and compiled to EVM bytecode. Contract functions are <em>stateful</em>: they have private storage on the blockchain, and can also hold some amount of virtual Ether coins. The private storage is allocated and initialized by running a constructor, subsequent transactions sent to the contract address invoke the anonymous function.</p>
<p>Here&#8217;s an example Puzzle contract:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/smart-contracts-fig-3.jpeg?w=640" alt="" /></p>
<p>Note the contract state declared on lines 2-6, constructor on lines 8-13, and anonymous transaction function on lines 15-29.  A default input variable <code>msg</code> holds the sender, amount of Ether sent to the contract, and any included data as part of the invocation. In this particular contract, if the owner initiates the transaction (line 16) they can extract the current reward value and replace it with some other amount (lines 17-21). Anyone else invoking the transaction can submit a potential solution, and will receive the reward if the solution is accepted (lines 23-29).</p>
<p>All miners execute the transaction, which will incur some computation cost:</p>
<blockquote><p>
  Ethereum pays miners some fees proportional to the required computation. Specifically, each instruction in the Ethereum bytecode has a pre-specified amount of gas. When a user sends a transaction to invoke a contract, she has to specify how much gas she is willing to provide for the execution (called gasLimit) as well as the price for each gas unit (called gasPrice). A miner who includes the transaction in his proposed block subsequently receives the transaction fee corresponding to the amount of gas the execution actually burns multiplied by gasPrice.
</p></blockquote>
<p>If the execution costs more than the gasLimit then execution is terminated and the state is restored to the initial state at the start of the function execution. The miner still receives <code>gasLimit</code> compensation though.</p>
<h3>Why are smart contracts attractive targets?</h3>
<p>Smart contracts have associated value &#8211; potentially handling large numbers of coins worth hundreds of dollars apiece. The 8,833 contracts in the first 1,460,000 blocks in the Ethereum network had a total balance of over 3 million Ether (about $30M USD) at the time the paper was written. The <a href="http://www.coindesk.com/understanding-dao-hack-journalists/">infamous attack on &#8216;TheDAO&#8217;</a> caused a loss of about $60M to TheDAO&#8217;s investors.</p>
<h3>Smart contract vulnerabilities</h3>
<p>So we know that smart contracts have value as attack targets. They also have a combination of features that should make any experienced software developer raise an eyebrow:</p>
<ul>
<li>They execute in permissionless networks which arbitrary participants can join (i.e., under byzantine conditions)</li>
<li>Miners and/or callers have meaningful control over the environment in which the transactions execute (which transactions to accept, transaction ordering, setting of block timestamp, manipulation of call stack)</li>
<li>All of the above must be reasoned about in an environment which punishes anyone who doesn&#8217;t get it right first time &#8211; there is no patching mechanism:</li>
</ul>
<blockquote><p>
  There is no way to patch a buggy smart contract, regardless of its popularity or how much money it has, without reversing the blockchain (a formidable task). Therefore, reasoning about the correctness of smart contracts before deployment is critical, as is designing a safe smart contract system.
</p></blockquote>
<p>Note: you <em>can</em> explicitly design versioning/upgrade capabilities into your smart contract code, since contracts can call each other. See e.g., <a href="http://ethereum.stackexchange.com/questions/2404/upgradeable-smart-contracts">http://ethereum.stackexchange.com/questions/2404/upgradeable-smart-contracts</a>. But it remains the case that the original bytecode associated with the contract is immutable for all time.</p>
<p>The authors discuss four major categories of vulnerabilities in smart contracts: <em>transaction-ordering dependence, timestamp dependence, mishandled exceptions</em>, and <em>reentrancy vulnerability</em>.</p>
<p>Miners can control the order in which transactions are executed, in particular this means that the state of a contract at the time a user submits a transaction may not match the state of the contract at the time the transaction executes if another transaction updates it first (version numbering and optimistic concurrency control anyone?). In the Puzzle example, someone may submit a solution hoping for the big reward, and the owner can nip in with another transaction that replaces it with little or no reward, so benefiting from the solution without paying out.</p>
<p>Some contracts use the block timestamp as a triggering condition or source of randomness (don&#8217;t do this!).</p>
<blockquote><p>
  Let us recall that when mining a block, a miner has to set the timestamp for the block (Figure 2). Normally, the timestamp is set as the current time of the miner’s local system. However, the miner can vary this value by roughly 900 seconds, while still having other miners accept the block&#8230; Thus, the adversary can choose different block timestamps to manipulate the outcome of timestamp-dependent contracts.
</p></blockquote>
<p>Ethereum contracts can call each other, but exceptions in the callee contract may not be propagated to the caller (depending on exact circumstances). &#8220;<em>This inconsistent exception propagation policy leads to many cases where exceptions are not handled properly.</em>&#8221; An adversary can load the dice by preparing a contract which calls itself 1023 times before calling the target contract. The Ethereum virtual machine has a 1024 call stack depth limit. Filling the call stack in this way means that the next call the target contract makes will fail with an exception. There is no atomicity here &#8211; any actions taken in the target contract directly will be preserved, but those taken by the contract it called will not. For example, ownership of a resource may be transferred, with payment being made.</p>
<p>If you had to guess what else might cause problems beyond error and exception handling, concurrency related bugs is always a good option. And indeed that turns out to be the case leading to <em>reentrancy vulnerabilities</em>:</p>
<blockquote><p>
  In Ethereum, when a contract calls another, the current execution waits for the call to finish. This can lead to an issue when the recipient of the call makes use of the intermediate state the caller is in. This may not be immediately obvious when writing the contract if possible malicious behavior on the side of the callee is not considered.
</p></blockquote>
<p>This example contract exhibits the vulnerability:</p>
<p><img src="https://adriancolyer.files.wordpress.com/2017/02/smart-contracts-fig-7.jpeg?w=640" alt="" /></p>
<p>Line 11 sends the current balance to the contract address wishing to withdraw its balance, but the balance is not zeroed until after the call (line 13). The callee contract can call back into withdrawBalance again and make multiple withdrawals in this manner.</p>
<h3>Protecting smart contracts</h3>
<p>The proposal to defend against transaction ordering is to allow a guard clause (predicate) to be evaluated before transaction execution. If the guard clause evaluates to false the transaction will not execute. Using this, you can roll-your-own optimistic concurrency scheme.</p>
<p>The solution to timestamp dependency is not to depend on timestamps &#8211; there are better sources of both randomness and timestamps available. &#8220;A practical fix (for the latter) is to translate existing notions of timestamp into block numbers.&#8221;</p>
<p>For exception handling, the straightforward solution is to check the return value whenever one contract calls another! If clients upgrade, an even better solution is to propagate exceptions at the level of the EVM from callee to caller and revert the state of the caller if they are not properly handled.</p>
<p>Based on a model of the operational semantics of the Ethereum bytecode (worth the price of admission all by itself, and we don&#8217;t even have the space to cover it here at all!), the authors build a verification tool called OYENTE which can symbolically execute contracts and look for vulnerabilities. OYENT is 4,00 lines of Python, and uses Z3 as the solver to decide satisfiability.</p>
<h3>What OYENTE discovered</h3>
<blockquote><p>
  We collected 19,366 smart contracts from the blockchain as of May 5, 2016. These contracts currently hold a total balance of 3,068,654 Ether, or 30 Million US dollars at the time of writing&#8230; On an average, a contract has 318.5 Ether, or equivalently 4523 US dollars.
</p></blockquote>
<p>8,833 of the contracts have at least one security issue: 5,411 contracts (27.9%) have mishandled exceptions; 3,056 contracts (15.7%) have transaction-ordering dependencies; 83 contracts have timestamp dependencies, and 340 contracts have reentrancy handling problems &#8211; one of which is the infamous TheDAO contract. You can see several examples of found vulnerabilities in section 6 of the paper.</p><img alt="" border="0" src="https://pixel.wp.com/b.gif?host=blog.acolyer.org&#038;blog=23592848&#038;post=3968&#038;subd=adriancolyer&#038;ref=&#038;feed=1" width="1" height="1" />]]></content:encoded>
            <wfw:commentRss>https://blog.acolyer.org/2017/02/23/making-smart-contracts-smarter/feed/</wfw:commentRss>
            <slash:comments>1</slash:comments>

            <media:content url="http://1.gravatar.com/avatar/a795b4f89a6d096f314fc0a2c80479c1?s=96&#38;d=identicon&#38;r=G" medium="image">
                <media:title type="html">adriancolyer</media:title>
            </media:content>

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/smart-contracts-fig-3.jpeg?w=640" medium="image" />

            <media:content url="http://adriancolyer.files.wordpress.com/2017/02/smart-contracts-fig-7.jpeg?w=640" medium="image" />
        </item>
    </channel>
</rss>
